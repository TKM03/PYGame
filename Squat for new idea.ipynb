{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "178009bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 23:37:47,425 - INFO - Pose detector initialized successfully\n",
      "2024-11-16 23:37:50,235 - INFO - Video capture started\n",
      "C:\\Users\\kitmi\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "2024-11-16 23:37:50,614 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-16 23:37:50,684 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-16 23:37:50,738 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-16 23:37:50,806 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-16 23:37:50,873 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-16 23:37:50,933 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-16 23:37:50,997 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-16 23:37:51,059 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-16 23:37:51,121 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-16 23:37:51,183 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-16 23:37:51,247 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-16 23:37:51,308 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-16 23:37:51,372 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:51,435 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:51,501 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:51,569 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:51,625 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:51,688 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:51,747 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:51,812 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:51,875 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:51,936 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:51,990 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:52,047 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:52,101 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:52,154 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:52,220 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:52,284 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:52,345 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:52,406 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:52,472 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:52,535 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:52,596 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:52,659 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:52,721 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-16 23:37:52,722 - INFO - User requested exit\n",
      "2024-11-16 23:37:53,315 - INFO - Analytics report generated\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import json\n",
    "import threading\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "class PoseAnalytics:\n",
    "    \"\"\"Analytics component for tracking and analyzing pose detection metrics\"\"\"\n",
    "    def __init__(self, save_dir='analytics'):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Initialize analytics storage\n",
    "        self.session_data = {\n",
    "            'timestamps': [],\n",
    "            'postures': [],\n",
    "            'model_used': [],\n",
    "            'confidence_scores': [],\n",
    "            'processing_times': [],\n",
    "            'fps_values': []\n",
    "        }\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            filename=self.save_dir / 'pose_detection.log',\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        \n",
    "        # Initialize real-time metrics\n",
    "        self.current_metrics = {\n",
    "            'session_start': datetime.now(),\n",
    "            'poses_detected': 0,\n",
    "            'model_switches': 0,\n",
    "            'average_fps': 0.0\n",
    "        }\n",
    "    \n",
    "    def update_metrics(self, posture, model, confidence, process_time, fps):\n",
    "        \"\"\"Update analytics with new frame data\"\"\"\n",
    "        timestamp = datetime.now()\n",
    "        \n",
    "        # Update session data\n",
    "        self.session_data['timestamps'].append(timestamp)\n",
    "        self.session_data['postures'].append(posture)\n",
    "        self.session_data['model_used'].append(model)\n",
    "        self.session_data['confidence_scores'].append(confidence)\n",
    "        self.session_data['processing_times'].append(process_time)\n",
    "        self.session_data['fps_values'].append(fps)\n",
    "        \n",
    "        # Update real-time metrics\n",
    "        self.current_metrics['poses_detected'] += 1\n",
    "        self.current_metrics['average_fps'] = np.mean(self.session_data['fps_values'])\n",
    "        \n",
    "        # Log significant events\n",
    "        if confidence < 0.5:\n",
    "            logging.warning(f\"Low confidence detection: {confidence:.2f}\")\n",
    "    \n",
    "    def _create_posture_distribution_plot(self, df):\n",
    "        \"\"\"Create and save posture distribution plot\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        posture_counts = df['postures'].value_counts()\n",
    "        sns.barplot(x=posture_counts.values, y=posture_counts.index)\n",
    "        plt.title('Posture Distribution')\n",
    "        plt.xlabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.save_dir / 'posture_distribution.png')\n",
    "        plt.close()\n",
    "\n",
    "    def _create_performance_plot(self, df):\n",
    "        \"\"\"Create and save performance metrics plot\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df['timestamps'], df['fps_values'], label='FPS')\n",
    "        plt.plot(df['timestamps'], df['processing_times'], label='Processing Time')\n",
    "        plt.title('Performance Metrics Over Time')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.save_dir / 'performance_metrics.png')\n",
    "        plt.close()\n",
    "\n",
    "    def _create_confidence_plot(self, df):\n",
    "        \"\"\"Create and save confidence scores plot\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df['timestamps'], df['confidence_scores'])\n",
    "        plt.title('Detection Confidence Over Time')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Confidence Score')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.save_dir / 'confidence_scores.png')\n",
    "        plt.close()\n",
    "\n",
    "    def generate_analytics_report(self):\n",
    "        \"\"\"Generate comprehensive analytics report\"\"\"\n",
    "        if not self.session_data['timestamps']:\n",
    "            logging.warning(\"No data collected for analytics report\")\n",
    "            return\n",
    "\n",
    "        # Convert data to DataFrame\n",
    "        df = pd.DataFrame(self.session_data)\n",
    "        \n",
    "        # Generate visualizations\n",
    "        self._create_posture_distribution_plot(df)\n",
    "        self._create_performance_plot(df)\n",
    "        self._create_confidence_plot(df)\n",
    "        \n",
    "        # Generate summary statistics\n",
    "        summary = self._generate_summary_stats(df)\n",
    "        \n",
    "        # Save report\n",
    "        self._save_analytics_report(df, summary)\n",
    "    \n",
    "    def _generate_summary_stats(self, df):\n",
    "        \"\"\"Generate summary statistics from the session data\"\"\"\n",
    "        summary = {\n",
    "            'total_frames': len(df),\n",
    "            'average_fps': df['fps_values'].mean(),\n",
    "            'average_confidence': df['confidence_scores'].mean(),\n",
    "            'most_common_posture': df['postures'].mode().iloc[0] if not df['postures'].empty else 'None',\n",
    "            'session_duration': str(datetime.now() - self.current_metrics['session_start'])\n",
    "        }\n",
    "        return summary\n",
    "\n",
    "    def _save_analytics_report(self, df, summary):\n",
    "        \"\"\"Save analytics report to file\"\"\"\n",
    "        report_path = self.save_dir / f'analytics_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt'\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"=== Pose Detection Analytics Report ===\\n\\n\")\n",
    "            for key, value in summary.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "class PoseDetector:\n",
    "    def __init__(self):\n",
    "        # MediaPipe and YOLO initialization\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.7,\n",
    "            model_complexity=2\n",
    "        )\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_drawing_styles = mp.solutions.drawing_styles\n",
    "        self.yolo_model = YOLO('yolov8n-pose.pt')\n",
    "        \n",
    "        # Initialize analytics\n",
    "        self.analytics = PoseAnalytics()\n",
    "        \n",
    "        # Performance monitoring\n",
    "        self.fps_history = deque(maxlen=30)\n",
    "        self.detection_history = deque(maxlen=30)\n",
    "        self.joint_consistency = deque(maxlen=30)\n",
    "        \n",
    "        # Model switching parameters\n",
    "        self.current_model = 'mediapipe'\n",
    "        self.last_switch_time = time.time()\n",
    "        self.switch_cooldown = 10\n",
    "        self.switch_pending = False\n",
    "        \n",
    "        # Initialize video recording\n",
    "        self.recording = False\n",
    "        self.video_writer = None\n",
    "        self.output_dir = Path('output')\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # State tracking\n",
    "        self.current_posture = \"No pose detected\"\n",
    "        self.current_confidence = 0.0\n",
    "        self.current_landmarks = None\n",
    "        \n",
    "        # Motion tracking\n",
    "        self.pose_history = deque(maxlen=5)\n",
    "        self.gesture_patterns = self._load_gesture_patterns()\n",
    "        \n",
    "        # Threshold values\n",
    "        self.movement_threshold = 10\n",
    "        self.shoulder_movement_threshold = 5\n",
    "        self.squat_threshold = 0.25\n",
    "        self.standing_hip_knee_ratio = 0.9\n",
    "        self.sitting_hip_knee_ratio = 0.4\n",
    "        self.arm_raised_threshold = 0.2\n",
    "        \n",
    "        self.current_frame_shape = None\n",
    "        \n",
    "    def process_frame(self, frame):\n",
    "            \"\"\"Process a single frame for pose detection with improved error handling\"\"\"\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                if frame is None or frame.size == 0:\n",
    "                    logging.error(\"Invalid frame received\")\n",
    "                    return frame\n",
    "\n",
    "                self.current_frame_shape = frame.shape\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Process with current model\n",
    "                landmarks = None\n",
    "                confidence = 0.0\n",
    "\n",
    "                try:\n",
    "                    if self.current_model == 'mediapipe':\n",
    "                        results = self.pose.process(rgb_frame)\n",
    "                        if results.pose_landmarks:\n",
    "                            landmarks = results.pose_landmarks\n",
    "                            confidence = np.mean([lm.visibility for lm in landmarks.landmark])\n",
    "                    else:  # YOLO\n",
    "                        results = self.yolo_model(rgb_frame)[0]\n",
    "                        if len(results.keypoints) > 0:\n",
    "                            # Safely extract keypoints\n",
    "                            try:\n",
    "                                keypoints = results.keypoints[0].data.cpu().numpy()\n",
    "                                if len(keypoints) > 0:\n",
    "                                    landmarks = keypoints\n",
    "                                    confidence = float(results.boxes[0].conf) if len(results.boxes) > 0 else 0.0\n",
    "                            except (IndexError, AttributeError) as e:\n",
    "                                logging.warning(f\"Error processing YOLO keypoints: {str(e)}\")\n",
    "                                landmarks = None\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error during pose detection: {str(e)}\")\n",
    "                    return frame\n",
    "\n",
    "                # Calculate metrics\n",
    "                process_time = time.time() - start_time\n",
    "                fps = 1 / process_time if process_time > 0 else 0\n",
    "                self.fps_history.append(fps)\n",
    "\n",
    "                if landmarks is not None:\n",
    "                    try:\n",
    "                        # Detect posture with safe conversion\n",
    "                        posture = self.detect_posture(landmarks, self.current_model)\n",
    "                        self.current_posture = posture\n",
    "                        self.current_confidence = confidence\n",
    "                        self.current_landmarks = landmarks\n",
    "\n",
    "                        # Calculate bounding box with error handling\n",
    "                        try:\n",
    "                            bbox = self.calculate_bounding_box(landmarks, self.current_model)\n",
    "                            self.draw_action_box(frame, bbox, posture)\n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Error drawing bounding box: {str(e)}\")\n",
    "\n",
    "                        # Draw landmarks based on model type\n",
    "                        if self.current_model == 'mediapipe':\n",
    "                            self.mp_drawing.draw_landmarks(\n",
    "                                frame,\n",
    "                                landmarks,\n",
    "                                self.mp_pose.POSE_CONNECTIONS,\n",
    "                                landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "                            )\n",
    "                        else:\n",
    "                            self.draw_yolo_skeleton(frame, landmarks)\n",
    "\n",
    "                        # Update analytics\n",
    "                        self.analytics.update_metrics(\n",
    "                            posture=posture,\n",
    "                            model=self.current_model,\n",
    "                            confidence=confidence,\n",
    "                            process_time=process_time,\n",
    "                            fps=fps\n",
    "                        )\n",
    "\n",
    "                        # Check for model switching\n",
    "                        self._check_model_switch(confidence)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing landmarks: {str(e)}\")\n",
    "\n",
    "                # Draw performance metrics\n",
    "                self._draw_performance_metrics(frame, fps, confidence)\n",
    "\n",
    "                # Handle recording if active\n",
    "                if self.recording and self.video_writer is not None:\n",
    "                    try:\n",
    "                        self.video_writer.write(frame)\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error writing video frame: {str(e)}\")\n",
    "\n",
    "                return frame\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error in process_frame: {str(e)}\")\n",
    "                return frame\n",
    "\n",
    "    def draw_yolo_skeleton(self, frame, keypoints):\n",
    "        \"\"\"Draw YOLO skeleton on frame with improved error handling\"\"\"\n",
    "        if not isinstance(keypoints, np.ndarray):\n",
    "            try:\n",
    "                keypoints = keypoints.cpu().numpy()\n",
    "            except AttributeError:\n",
    "                logging.error(\"Invalid keypoints format\")\n",
    "                return\n",
    "\n",
    "        connections = [\n",
    "            (0, 1), (1, 2), (2, 3), (3, 4),  # Right arm\n",
    "            (5, 6), (6, 7), (7, 8),          # Left arm\n",
    "            (9, 10), (11, 12),               # Spine\n",
    "            (12, 13), (13, 14), (14, 15),    # Right leg\n",
    "            (16, 17), (17, 18), (18, 19)     # Left leg\n",
    "        ]\n",
    "\n",
    "        # Ensure keypoints are within frame boundaries\n",
    "        h, w = frame.shape[:2]\n",
    "        \n",
    "        for start, end in connections:\n",
    "            try:\n",
    "                if start >= len(keypoints) or end >= len(keypoints):\n",
    "                    continue\n",
    "                    \n",
    "                x1, y1, conf1 = keypoints[start]\n",
    "                x2, y2, conf2 = keypoints[end]\n",
    "\n",
    "                # Clip coordinates to frame boundaries\n",
    "                x1, y1 = np.clip(int(x1), 0, w-1), np.clip(int(y1), 0, h-1)\n",
    "                x2, y2 = np.clip(int(x2), 0, w-1), np.clip(int(y2), 0, h-1)\n",
    "\n",
    "                if conf1 > 0.5 and conf2 > 0.5:  # Only draw if confidence is sufficient\n",
    "                    cv2.line(frame, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "\n",
    "            except (IndexError, ValueError) as e:\n",
    "                continue\n",
    "\n",
    "        # Draw keypoints\n",
    "        for kp in keypoints:\n",
    "            try:\n",
    "                x, y, conf = kp\n",
    "                if conf > 0.5:\n",
    "                    x, y = np.clip(int(x), 0, w-1), np.clip(int(y), 0, h-1)\n",
    "                    cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)\n",
    "            except (IndexError, ValueError):\n",
    "                continue\n",
    "\n",
    "\n",
    "    def _check_model_switch(self, confidence):\n",
    "        \"\"\"Check if model switch is needed based on performance\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Only consider switching if cooldown period has passed\n",
    "        if current_time - self.last_switch_time < self.switch_cooldown:\n",
    "            return\n",
    "            \n",
    "        # Switch models if confidence is consistently low\n",
    "        if confidence < 0.5:\n",
    "            self.detection_history.append(False)\n",
    "        else:\n",
    "            self.detection_history.append(True)\n",
    "            \n",
    "        # Calculate detection rate\n",
    "        detection_rate = sum(self.detection_history) / len(self.detection_history)\n",
    "        \n",
    "        if detection_rate < 0.7 and not self.switch_pending:\n",
    "            self.switch_pending = True\n",
    "            self.current_model = 'yolo' if self.current_model == 'mediapipe' else 'mediapipe'\n",
    "            self.last_switch_time = current_time\n",
    "            logging.info(f\"Switching to {self.current_model} model\")\n",
    "            self.switch_pending = False\n",
    "\n",
    "    def _check_gestures(self):\n",
    "        \"\"\"Check for gesture patterns in pose history\"\"\"\n",
    "        if len(self.pose_history) < 3:\n",
    "            return\n",
    "            \n",
    "        # Convert history to list for pattern matching\n",
    "        history_list = list(self.pose_history)\n",
    "        \n",
    "        # Check each gesture pattern\n",
    "        for gesture, pattern in self.gesture_patterns.items():\n",
    "            sequence = pattern['sequence']\n",
    "            if len(history_list) >= len(sequence):\n",
    "                # Check last n poses against pattern\n",
    "                last_n_poses = history_list[-len(sequence):]\n",
    "                if all(a == b for a, b in zip(last_n_poses, sequence)):\n",
    "                    logging.info(f\"Gesture detected: {gesture}\")\n",
    "\n",
    "    def _draw_performance_metrics(self, frame, fps, confidence):\n",
    "        \"\"\"Draw performance metrics on frame\"\"\"\n",
    "        # Calculate average FPS\n",
    "        avg_fps = np.mean(self.fps_history) if self.fps_history else 0\n",
    "        \n",
    "        # Draw metrics\n",
    "        metrics_text = [\n",
    "            f\"FPS: {avg_fps:.1f}\",\n",
    "            f\"Model: {self.current_model}\",\n",
    "            f\"Confidence: {confidence:.2f}\",\n",
    "            f\"Posture: {self.current_posture}\"\n",
    "        ]\n",
    "        \n",
    "        y_position = 30\n",
    "        for text in metrics_text:\n",
    "            cv2.putText(frame, text, (10, y_position),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            y_position += 25\n",
    "\n",
    "    def toggle_recording(self):\n",
    "        \"\"\"Toggle video recording\"\"\"\n",
    "        if not self.recording:\n",
    "            # Start recording\n",
    "            filename = self.output_dir / f'pose_recording_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.avi'\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "            self.video_writer = cv2.VideoWriter(str(filename), fourcc, 20.0, \n",
    "                                              (self.current_frame_shape[1], self.current_frame_shape[0]))\n",
    "            self.recording = True\n",
    "            logging.info(\"Recording started\")\n",
    "        else:\n",
    "            # Stop recording\n",
    "            if self.video_writer is not None:\n",
    "                self.video_writer.release()\n",
    "                self.video_writer = None\n",
    "            self.recording = False\n",
    "            logging.info(\"Recording stopped\")\n",
    "\n",
    "    def save_frame(self):\n",
    "        \"\"\"Save current frame as image\"\"\"\n",
    "        if self.current_frame_shape is not None:\n",
    "            filename = self.output_dir / f'pose_frame_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.jpg'\n",
    "            cv2.imwrite(str(filename), self.current_frame)\n",
    "            logging.info(f\"Frame saved: {filename}\")\n",
    "\n",
    "    def _load_gesture_patterns(self):\n",
    "        \"\"\"Load predefined gesture patterns\"\"\"\n",
    "        return {\n",
    "            'wave': {'sequence': ['standing with raised arms', 'standing'], 'threshold': 0.8},\n",
    "            'squat': {'sequence': ['standing', 'squatting', 'standing'], 'threshold': 0.9}\n",
    "        }\n",
    "\n",
    "    def detect_posture(self, landmarks, model_type):\n",
    "        \"\"\"Detect posture from landmarks\"\"\"\n",
    "        if model_type == 'mediapipe':\n",
    "            return self._detect_posture_mediapipe(landmarks)\n",
    "        else:\n",
    "            return self._detect_posture_yolo(landmarks)\n",
    "\n",
    "    def _detect_posture_mediapipe(self, landmarks):\n",
    "        \"\"\"Detect posture using MediaPipe landmarks\"\"\"\n",
    "        # Get relevant landmark points\n",
    "        nose = landmarks.landmark[self.mp_pose.PoseLandmark.NOSE]\n",
    "        left_shoulder = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        left_hip = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_HIP]\n",
    "        right_hip = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_HIP]\n",
    "        left_knee = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_KNEE]\n",
    "        right_knee = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_KNEE]\n",
    "        \n",
    "        # Calculate angles and ratios\n",
    "        hip_knee_ratio = self._calculate_hip_knee_ratio(left_hip, right_hip, left_knee, right_knee)\n",
    "        shoulder_hip_angle = self._calculate_angle(\n",
    "            (left_shoulder.x, left_shoulder.y),\n",
    "            (left_hip.x, left_hip.y),\n",
    "            (left_knee.x, left_knee.y)\n",
    "        )\n",
    "        \n",
    "        # Detect posture based on calculations\n",
    "        if hip_knee_ratio > self.standing_hip_knee_ratio:\n",
    "            if self._are_arms_raised(landmarks):\n",
    "                return \"standing with raised arms\"\n",
    "            return \"standing\"\n",
    "        elif hip_knee_ratio < self.sitting_hip_knee_ratio:\n",
    "            return \"sitting\"\n",
    "        elif shoulder_hip_angle < 150:  # degrees\n",
    "            return \"squatting\"\n",
    "        else:\n",
    "            return \"undefined posture\"\n",
    "\n",
    "    def _detect_posture_yolo(self, landmarks):\n",
    "        \"\"\"Detect posture using YOLO landmarks\"\"\"\n",
    "        # Convert YOLO format to similar structure as MediaPipe for reuse\n",
    "        converted_landmarks = self._convert_yolo_to_mediapipe_format(landmarks)\n",
    "        return self._detect_posture_mediapipe(converted_landmarks)\n",
    "\n",
    "    def _convert_yolo_to_mediapipe_format(self, yolo_landmarks):\n",
    "        \"\"\"Convert YOLO landmark format to MediaPipe-like format\"\"\"\n",
    "        # Create a simple class to mimic MediaPipe landmark structure\n",
    "        class LandmarkContainer:\n",
    "            def __init__(self, landmarks):\n",
    "                self.landmark = landmarks\n",
    "\n",
    "        # Convert YOLO keypoints to normalized coordinates\n",
    "        converted = []\n",
    "        # YOLO keypoints come as a tensor, need to convert to numpy array first\n",
    "        keypoints_array = yolo_landmarks.cpu().numpy() if hasattr(yolo_landmarks, 'cpu') else yolo_landmarks\n",
    "    \n",
    "        if isinstance(keypoints_array, np.ndarray):\n",
    "            # If it's a batch of keypoints, take the first person\n",
    "            if len(keypoints_array.shape) == 3:\n",
    "                keypoints_array = keypoints_array[0]\n",
    "            \n",
    "            for point in keypoints_array:\n",
    "                if len(point) >= 2:  # Ensure we have at least x,y coordinates\n",
    "                    landmark = type('Landmark', (), {\n",
    "                        'x': float(point[0]) / float(self.current_frame_shape[1]),\n",
    "                        'y': float(point[1]) / float(self.current_frame_shape[0]),\n",
    "                        'visibility': float(point[2]) if len(point) > 2 else 1.0\n",
    "                    })\n",
    "                    converted.append(landmark)\n",
    "        \n",
    "        return LandmarkContainer(converted)\n",
    "\n",
    "    def _calculate_hip_knee_ratio(self, left_hip, right_hip, left_knee, right_knee):\n",
    "        \"\"\"Calculate the ratio between hip and knee positions\"\"\"\n",
    "        hip_y = (left_hip.y + right_hip.y) / 2\n",
    "        knee_y = (left_knee.y + right_knee.y) / 2\n",
    "        return abs(knee_y - hip_y)\n",
    "\n",
    "    def _calculate_angle(self, p1, p2, p3):\n",
    "        \"\"\"Calculate angle between three points\"\"\"\n",
    "        v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])\n",
    "        v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])\n",
    "        \n",
    "        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n",
    "        return np.degrees(angle)\n",
    "\n",
    "    def _are_arms_raised(self, landmarks):\n",
    "        \"\"\"Check if arms are raised above shoulders\"\"\"\n",
    "        left_wrist = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "        right_wrist = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_WRIST]\n",
    "        left_shoulder = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        \n",
    "        return (left_wrist.y < left_shoulder.y - self.arm_raised_threshold or \n",
    "                right_wrist.y < right_shoulder.y - self.arm_raised_threshold)\n",
    "\n",
    "    def calculate_bounding_box(self, landmarks, model_type):\n",
    "        \"\"\"Calculate bounding box for detected pose\"\"\"\n",
    "        if model_type == 'mediapipe':\n",
    "            points = [(lm.x, lm.y) for lm in landmarks.landmark]\n",
    "        else:  # YOLO\n",
    "            points = [(x/self.current_frame_shape[1], y/self.current_frame_shape[0]) \n",
    "                     for x, y, _ in landmarks]\n",
    "        \n",
    "        x_coords = [p[0] for p in points]\n",
    "        y_coords = [p[1] for p in points]\n",
    "        \n",
    "        # Calculate normalized coordinates\n",
    "        x_min, x_max = min(x_coords), max(x_coords)\n",
    "        y_min, y_max = min(y_coords), max(y_coords)\n",
    "        \n",
    "        # Convert to pixel coordinates\n",
    "        return {\n",
    "            'x1': int(x_min * self.current_frame_shape[1]),\n",
    "            'y1': int(y_min * self.current_frame_shape[0]),\n",
    "            'x2': int(x_max * self.current_frame_shape[1]),\n",
    "            'y2': int(y_max * self.current_frame_shape[0])\n",
    "        }\n",
    "\n",
    "    def draw_action_box(self, frame, bbox, action):\n",
    "        \"\"\"Draw bounding box and action label on frame\"\"\"\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, \n",
    "                     (bbox['x1'], bbox['y1']), \n",
    "                     (bbox['x2'], bbox['y2']), \n",
    "                     (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw action label\n",
    "        label_size = cv2.getTextSize(action, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n",
    "        cv2.rectangle(frame,\n",
    "                     (bbox['x1'], bbox['y1'] - label_size[1] - 10),\n",
    "                     (bbox['x1'] + label_size[0], bbox['y1']),\n",
    "                     (0, 255, 0), -1)\n",
    "        cv2.putText(frame, action,\n",
    "                   (bbox['x1'], bbox['y1'] - 5),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Enhanced main function with analytics and error handling\"\"\"\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Initialize detector\n",
    "        detector = PoseDetector()\n",
    "        logging.info(\"Pose detector initialized successfully\")\n",
    "        \n",
    "        # Initialize video capture\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            raise RuntimeError(\"Could not open webcam\")\n",
    "        \n",
    "        logging.info(\"Video capture started\")\n",
    "        \n",
    "        # Main processing loop\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                logging.warning(\"Failed to capture frame\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Process frame\n",
    "                processed_frame = detector.process_frame(frame)\n",
    "                if processed_frame is None:\n",
    "                    continue\n",
    "                \n",
    "                # Display frame\n",
    "                cv2.imshow('Pose Detection', processed_frame)\n",
    "                \n",
    "                # Handle keyboard input\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    logging.info(\"User requested exit\")\n",
    "                    break\n",
    "                elif key == ord('r'):\n",
    "                    detector.toggle_recording()\n",
    "                elif key == ord('s'):\n",
    "                    detector.save_frame()\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing frame: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fatal error: {str(e)}\")\n",
    "        \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        if cap is not None:\n",
    "            cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Generate final analytics\n",
    "        detector.analytics.generate_analytics_report()\n",
    "        logging.info(\"Analytics report generated\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236ef74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 00:06:27,153 - INFO - Pose detector initialized successfully\n",
      "2024-11-17 00:06:29,822 - INFO - Video capture started\n",
      "C:\\Users\\kitmi\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "2024-11-17 00:06:30,388 - WARNING - Low confidence detection: 0.50\n",
      "2024-11-17 00:06:30,782 - WARNING - Low confidence detection: 0.50\n",
      "2024-11-17 00:06:31,285 - WARNING - Low confidence detection: 0.50\n",
      "2024-11-17 00:06:31,349 - WARNING - Low confidence detection: 0.50\n",
      "2024-11-17 00:06:31,411 - WARNING - Low confidence detection: 0.50\n",
      "2024-11-17 00:06:31,476 - WARNING - Low confidence detection: 0.50\n",
      "2024-11-17 00:06:31,538 - WARNING - Low confidence detection: 0.50\n",
      "2024-11-17 00:06:31,600 - WARNING - Low confidence detection: 0.50\n",
      "2024-11-17 00:06:31,664 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:06:31,727 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:06:31,792 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:06:31,854 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:06:31,917 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:06:31,983 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:06:32,046 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:06:32,108 - WARNING - Low confidence detection: 0.48\n",
      "2024-11-17 00:06:32,172 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:06:32,234 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:06:32,297 - WARNING - Low confidence detection: 0.50\n",
      "2024-11-17 00:06:32,927 - WARNING - Low confidence detection: 0.48\n",
      "2024-11-17 00:06:32,992 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-17 00:06:33,052 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-17 00:06:33,116 - WARNING - Low confidence detection: 0.43\n",
      "2024-11-17 00:06:33,178 - WARNING - Low confidence detection: 0.42\n",
      "2024-11-17 00:06:33,242 - WARNING - Low confidence detection: 0.41\n",
      "2024-11-17 00:06:33,304 - WARNING - Low confidence detection: 0.41\n",
      "2024-11-17 00:06:33,370 - WARNING - Low confidence detection: 0.41\n",
      "2024-11-17 00:06:33,432 - WARNING - Low confidence detection: 0.42\n",
      "2024-11-17 00:06:33,494 - WARNING - Low confidence detection: 0.41\n",
      "2024-11-17 00:06:33,558 - WARNING - Low confidence detection: 0.40\n",
      "2024-11-17 00:06:33,622 - WARNING - Low confidence detection: 0.39\n",
      "2024-11-17 00:06:33,687 - WARNING - Low confidence detection: 0.39\n",
      "2024-11-17 00:06:33,748 - WARNING - Low confidence detection: 0.39\n",
      "2024-11-17 00:06:33,810 - WARNING - Low confidence detection: 0.40\n",
      "2024-11-17 00:06:33,873 - WARNING - Low confidence detection: 0.40\n",
      "2024-11-17 00:06:33,938 - WARNING - Low confidence detection: 0.40\n",
      "2024-11-17 00:06:34,001 - WARNING - Low confidence detection: 0.40\n",
      "2024-11-17 00:06:34,060 - WARNING - Low confidence detection: 0.39\n",
      "2024-11-17 00:06:34,126 - WARNING - Low confidence detection: 0.39\n",
      "2024-11-17 00:06:34,188 - WARNING - Low confidence detection: 0.40\n",
      "2024-11-17 00:06:34,251 - WARNING - Low confidence detection: 0.40\n",
      "2024-11-17 00:06:34,311 - WARNING - Low confidence detection: 0.39\n",
      "2024-11-17 00:06:34,374 - WARNING - Low confidence detection: 0.40\n",
      "2024-11-17 00:06:34,437 - WARNING - Low confidence detection: 0.42\n",
      "2024-11-17 00:06:34,500 - WARNING - Low confidence detection: 0.43\n",
      "2024-11-17 00:06:34,563 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-17 00:06:34,627 - WARNING - Low confidence detection: 0.47\n",
      "2024-11-17 00:06:34,688 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:06:34,751 - WARNING - Low confidence detection: 0.48\n",
      "2024-11-17 00:06:34,814 - WARNING - Low confidence detection: 0.47\n",
      "2024-11-17 00:06:34,877 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-17 00:06:34,937 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-17 00:06:35,000 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-17 00:06:35,062 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-17 00:06:35,125 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-17 00:06:35,188 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-17 00:06:35,249 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-17 00:06:35,314 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-17 00:06:35,374 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-17 00:06:35,437 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-17 00:06:35,500 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-17 00:06:35,562 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-17 00:06:35,627 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-17 00:06:35,690 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-17 00:06:35,752 - WARNING - Low confidence detection: 0.47\n",
      "2024-11-17 00:06:35,815 - WARNING - Low confidence detection: 0.47\n",
      "2024-11-17 00:06:35,878 - WARNING - Low confidence detection: 0.48\n",
      "2024-11-17 00:06:35,940 - WARNING - Low confidence detection: 0.47\n",
      "2024-11-17 00:06:36,003 - WARNING - Low confidence detection: 0.47\n",
      "2024-11-17 00:06:36,065 - WARNING - Low confidence detection: 0.48\n",
      "2024-11-17 00:06:36,130 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:06:36,191 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:06:36,255 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:06:36,316 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:06:36,382 - WARNING - Low confidence detection: 0.50\n",
      "2024-11-17 00:07:26,083 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:07:26,145 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:07:26,207 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:07:26,271 - WARNING - Low confidence detection: 0.48\n",
      "2024-11-17 00:07:26,331 - WARNING - Low confidence detection: 0.47\n",
      "2024-11-17 00:07:26,394 - WARNING - Low confidence detection: 0.47\n",
      "2024-11-17 00:07:26,455 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-17 00:07:26,518 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-17 00:07:26,580 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-17 00:07:26,643 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-17 00:07:26,705 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-17 00:07:26,767 - WARNING - Low confidence detection: 0.43\n",
      "2024-11-17 00:07:26,829 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-17 00:07:26,890 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-17 00:07:26,953 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-17 00:07:27,014 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-17 00:07:27,077 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-17 00:07:27,135 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-17 00:07:27,198 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-17 00:07:27,261 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-17 00:07:27,323 - WARNING - Low confidence detection: 0.47\n",
      "2024-11-17 00:07:27,387 - WARNING - Low confidence detection: 0.47\n",
      "2024-11-17 00:07:27,450 - WARNING - Low confidence detection: 0.47\n",
      "2024-11-17 00:07:27,513 - WARNING - Low confidence detection: 0.47\n",
      "2024-11-17 00:07:38,645 - WARNING - Low confidence detection: 0.48\n",
      "2024-11-17 00:07:38,707 - WARNING - Low confidence detection: 0.48\n",
      "2024-11-17 00:07:38,775 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-17 00:07:38,833 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-17 00:07:38,894 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-17 00:07:38,958 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-17 00:07:39,023 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-17 00:07:39,085 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-17 00:07:39,148 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-17 00:07:39,208 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-17 00:07:39,271 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-17 00:07:39,334 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-17 00:07:39,394 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-17 00:07:39,459 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-17 00:07:39,522 - WARNING - Low confidence detection: 0.43\n",
      "2024-11-17 00:07:39,583 - WARNING - Low confidence detection: 0.44\n",
      "2024-11-17 00:07:39,647 - WARNING - Low confidence detection: 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 00:07:39,712 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-17 00:07:39,773 - WARNING - Low confidence detection: 0.45\n",
      "2024-11-17 00:07:39,838 - WARNING - Low confidence detection: 0.46\n",
      "2024-11-17 00:07:39,898 - WARNING - Low confidence detection: 0.47\n",
      "2024-11-17 00:07:39,962 - WARNING - Low confidence detection: 0.48\n",
      "2024-11-17 00:07:40,032 - WARNING - Low confidence detection: 0.49\n",
      "2024-11-17 00:07:40,088 - WARNING - Low confidence detection: 0.49\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import json\n",
    "import threading\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import csv\n",
    "import time\n",
    "\n",
    "class PoseAnalytics:\n",
    "    \"\"\"Analytics component for tracking and analyzing pose detection metrics\"\"\"\n",
    "    def __init__(self, save_dir='analytics'):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Initialize analytics storage\n",
    "        self.session_data = {\n",
    "            'timestamps': [],\n",
    "            'postures': [],\n",
    "            'confidence_scores': [],\n",
    "            'processing_times': [],\n",
    "            'fps_values': []\n",
    "        }\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            filename=self.save_dir / 'pose_detection.log',\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        \n",
    "        # Initialize real-time metrics\n",
    "        self.current_metrics = {\n",
    "            'session_start': datetime.now(),\n",
    "            'poses_detected': 0,\n",
    "            'average_fps': 0.0\n",
    "        }\n",
    "    \n",
    "    def update_metrics(self, posture, confidence, process_time, fps):\n",
    "        \"\"\"Update analytics with new frame data\"\"\"\n",
    "        timestamp = datetime.now()\n",
    "        \n",
    "        # Update session data\n",
    "        self.session_data['timestamps'].append(timestamp)\n",
    "        self.session_data['postures'].append(posture)\n",
    "        self.session_data['confidence_scores'].append(confidence)\n",
    "        self.session_data['processing_times'].append(process_time)\n",
    "        self.session_data['fps_values'].append(fps)\n",
    "        \n",
    "        # Update real-time metrics\n",
    "        self.current_metrics['poses_detected'] += 1\n",
    "        self.current_metrics['average_fps'] = np.mean(self.session_data['fps_values'])\n",
    "        \n",
    "        # Log significant events\n",
    "        if confidence < 0.5:\n",
    "            logging.warning(f\"Low confidence detection: {confidence:.2f}\")\n",
    "    \n",
    "    def _create_posture_distribution_plot(self, df):\n",
    "        \"\"\"Create and save posture distribution plot\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        posture_counts = df['postures'].value_counts()\n",
    "        sns.barplot(x=posture_counts.values, y=posture_counts.index)\n",
    "        plt.title('Posture Distribution')\n",
    "        plt.xlabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.save_dir / 'posture_distribution.png')\n",
    "        plt.close()\n",
    "\n",
    "    def _create_performance_plot(self, df):\n",
    "        \"\"\"Create and save performance metrics plot\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df['timestamps'], df['fps_values'], label='FPS')\n",
    "        plt.plot(df['timestamps'], df['processing_times'], label='Processing Time')\n",
    "        plt.title('Performance Metrics Over Time')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.save_dir / 'performance_metrics.png')\n",
    "        plt.close()\n",
    "\n",
    "    def _create_confidence_plot(self, df):\n",
    "        \"\"\"Create and save confidence scores plot\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df['timestamps'], df['confidence_scores'])\n",
    "        plt.title('Detection Confidence Over Time')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Confidence Score')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.save_dir / 'confidence_scores.png')\n",
    "        plt.close()\n",
    "\n",
    "    def generate_analytics_report(self):\n",
    "        \"\"\"Generate comprehensive analytics report\"\"\"\n",
    "        if not self.session_data['timestamps']:\n",
    "            logging.warning(\"No data collected for analytics report\")\n",
    "            return\n",
    "\n",
    "        # Convert data to DataFrame\n",
    "        df = pd.DataFrame(self.session_data)\n",
    "        \n",
    "        # Generate visualizations\n",
    "        self._create_posture_distribution_plot(df)\n",
    "        self._create_performance_plot(df)\n",
    "        self._create_confidence_plot(df)\n",
    "        \n",
    "        # Generate summary statistics\n",
    "        summary = self._generate_summary_stats(df)\n",
    "        \n",
    "        # Save report\n",
    "        self._save_analytics_report(df, summary)\n",
    "    \n",
    "    def _generate_summary_stats(self, df):\n",
    "        \"\"\"Generate summary statistics from the session data\"\"\"\n",
    "        summary = {\n",
    "            'total_frames': len(df),\n",
    "            'average_fps': df['fps_values'].mean(),\n",
    "            'average_confidence': df['confidence_scores'].mean(),\n",
    "            'most_common_posture': df['postures'].mode().iloc[0] if not df['postures'].empty else 'None',\n",
    "            'session_duration': str(datetime.now() - self.current_metrics['session_start'])\n",
    "        }\n",
    "        return summary\n",
    "\n",
    "    def _save_analytics_report(self, df, summary):\n",
    "        \"\"\"Save analytics report to file\"\"\"\n",
    "        report_path = self.save_dir / f'analytics_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt'\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"=== Pose Detection Analytics Report ===\\n\\n\")\n",
    "            for key, value in summary.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "class PoseDetector:\n",
    "    def __init__(self):\n",
    "        # MediaPipe initialization\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.7,\n",
    "            model_complexity=2\n",
    "        )\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_drawing_styles = mp.solutions.drawing_styles\n",
    "        \n",
    "        # Initialize analytics\n",
    "        self.analytics = PoseAnalytics()\n",
    "        \n",
    "        # Performance monitoring\n",
    "        self.fps_history = deque(maxlen=30)\n",
    "        self.detection_history = deque(maxlen=30)\n",
    "        self.joint_consistency = deque(maxlen=30)\n",
    "        \n",
    "        # Initialize video recording\n",
    "        self.recording = False\n",
    "        self.video_writer = None\n",
    "        self.output_dir = Path('output')\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # State tracking\n",
    "        self.current_posture = \"No pose detected\"\n",
    "        self.current_confidence = 0.0\n",
    "        self.current_landmarks = None\n",
    "        \n",
    "        # Motion tracking\n",
    "        self.pose_history = deque(maxlen=5)\n",
    "        self.gesture_patterns = self._load_gesture_patterns()\n",
    "        \n",
    "        # Threshold values\n",
    "        self.movement_threshold = 10\n",
    "        self.shoulder_movement_threshold = 5\n",
    "        self.squat_threshold = 0.25\n",
    "        self.standing_hip_knee_ratio = 0.9\n",
    "        self.sitting_hip_knee_ratio = 0.4\n",
    "        self.arm_raised_threshold = 0.2\n",
    "        \n",
    "        self.current_frame_shape = None\n",
    "        \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process a single frame for pose detection with improved error handling\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            if frame is None or frame.size == 0:\n",
    "                logging.error(\"Invalid frame received\")\n",
    "                return frame\n",
    "\n",
    "            self.current_frame_shape = frame.shape\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Process with MediaPipe\n",
    "            results = self.pose.process(rgb_frame)\n",
    "            landmarks = None\n",
    "            confidence = 0.0\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                landmarks = results.pose_landmarks\n",
    "                confidence = np.mean([lm.visibility for lm in landmarks.landmark])\n",
    "\n",
    "                try:\n",
    "                    # Detect posture\n",
    "                    posture = self.detect_posture(landmarks)\n",
    "                    self.current_posture = posture\n",
    "                    self.current_confidence = confidence\n",
    "                    self.current_landmarks = landmarks\n",
    "\n",
    "                    # Calculate and draw bounding box\n",
    "                    bbox = self.calculate_bounding_box(landmarks, model_type=\"mediapipe\")\n",
    "                    self.draw_action_box(frame, bbox, posture)\n",
    "\n",
    "                    # Draw landmarks\n",
    "                    self.mp_drawing.draw_landmarks(\n",
    "                        frame,\n",
    "                        landmarks,\n",
    "                        self.mp_pose.POSE_CONNECTIONS,\n",
    "                        landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "                    )\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    process_time = time.time() - start_time\n",
    "                    fps = 1 / process_time if process_time > 0 else 0\n",
    "                    self.fps_history.append(fps)\n",
    "\n",
    "                    # Update analytics\n",
    "                    self.analytics.update_metrics(\n",
    "                        posture=posture,\n",
    "                        confidence=confidence,\n",
    "                        process_time=process_time,\n",
    "                        fps=fps\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing landmarks: {str(e)}\")\n",
    "\n",
    "\n",
    "            # Draw performance metrics\n",
    "            self._draw_performance_metrics(frame, fps if 'fps' in locals() else 0, confidence)\n",
    "\n",
    "            # Handle recording if active\n",
    "            if self.recording and self.video_writer is not None:\n",
    "                try:\n",
    "                    self.video_writer.write(frame)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error writing video frame: {str(e)}\")\n",
    "\n",
    "            return frame\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in process_frame: {str(e)}\")\n",
    "            return frame\n",
    "\n",
    "    def _draw_performance_metrics(self, frame, fps, confidence):\n",
    "        \"\"\"Draw performance metrics on frame\"\"\"\n",
    "        # Calculate average FPS\n",
    "        avg_fps = np.mean(self.fps_history) if self.fps_history else 0\n",
    "        \n",
    "        # Draw metrics\n",
    "        metrics_text = [\n",
    "            f\"FPS: {avg_fps:.1f}\",\n",
    "            f\"Confidence: {confidence:.2f}\",\n",
    "            f\"Posture: {self.current_posture}\"\n",
    "        ]\n",
    "        \n",
    "        y_position = 30\n",
    "        for text in metrics_text:\n",
    "            cv2.putText(frame, text, (10, y_position),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            y_position += 25\n",
    "\n",
    "    def toggle_recording(self):\n",
    "        \"\"\"Toggle video recording\"\"\"\n",
    "        if not self.recording:\n",
    "            # Start recording\n",
    "            filename = self.output_dir / f'pose_recording_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.avi'\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "            self.video_writer = cv2.VideoWriter(str(filename), fourcc, 20.0, \n",
    "                                              (self.current_frame_shape[1], self.current_frame_shape[0]))\n",
    "            self.recording = True\n",
    "            logging.info(\"Recording started\")\n",
    "        else:\n",
    "            # Stop recording\n",
    "            if self.video_writer is not None:\n",
    "                self.video_writer.release()\n",
    "                self.video_writer = None\n",
    "            self.recording = False\n",
    "            logging.info(\"Recording stopped\")\n",
    "\n",
    "    def save_frame(self):\n",
    "        \"\"\"Save current frame as image\"\"\"\n",
    "        if self.current_frame_shape is not None:\n",
    "            filename = self.output_dir / f'pose_frame_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.jpg'\n",
    "            cv2.imwrite(str(filename), self.current_frame)\n",
    "            logging.info(f\"Frame saved: {filename}\")\n",
    "\n",
    "    def _load_gesture_patterns(self):\n",
    "        \"\"\"Load predefined gesture patterns\"\"\"\n",
    "        return {\n",
    "            'wave': {'sequence': ['standing with raised arms', 'standing'], 'threshold': 0.8},\n",
    "            'squat': {'sequence': ['standing', 'squatting', 'standing'], 'threshold': 0.9}\n",
    "        }\n",
    "\n",
    "    def detect_posture(self, landmarks):\n",
    "        \"\"\"Enhanced posture detection with more accurate standing/sitting differentiation\"\"\"\n",
    "        # Get relevant landmark points\n",
    "        left_shoulder = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        left_hip = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_HIP]\n",
    "        right_hip = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_HIP]\n",
    "        left_knee = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_KNEE]\n",
    "        right_knee = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_KNEE]\n",
    "        left_ankle = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_ANKLE]\n",
    "        right_ankle = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_ANKLE]\n",
    "\n",
    "        # Calculate vertical alignments and angles\n",
    "        hip_y = (left_hip.y + right_hip.y) / 2\n",
    "        knee_y = (left_knee.y + right_knee.y) / 2\n",
    "        ankle_y = (left_ankle.y + right_ankle.y) / 2\n",
    "        shoulder_y = (left_shoulder.y + right_shoulder.y) / 2\n",
    "\n",
    "        # Calculate angles\n",
    "        left_knee_angle = self._calculate_angle(\n",
    "            (left_hip.x, left_hip.y),\n",
    "            (left_knee.x, left_knee.y),\n",
    "            (left_ankle.x, left_ankle.y)\n",
    "        )\n",
    "        right_knee_angle = self._calculate_angle(\n",
    "            (right_hip.x, right_hip.y),\n",
    "            (right_knee.x, right_knee.y),\n",
    "            (right_ankle.x, right_ankle.y)\n",
    "        )\n",
    "\n",
    "        # Calculate vertical alignment scores\n",
    "        spine_alignment = abs(shoulder_y - hip_y)  # Smaller means more upright\n",
    "        leg_alignment = abs(hip_y - ankle_y)       # Larger means more likely standing\n",
    "\n",
    "        # Enhanced standing detection\n",
    "        is_standing = (\n",
    "            knee_y > hip_y and                     # Knees below hips\n",
    "            ankle_y > knee_y and                   # Ankles below knees\n",
    "            spine_alignment < 0.2 and              # Relatively straight spine\n",
    "            leg_alignment > 0.3 and                # Legs extended\n",
    "            left_knee_angle > 160 and              # Straight legs\n",
    "            right_knee_angle > 160                 # Straight legs\n",
    "        )\n",
    "\n",
    "        # Enhanced sitting detection\n",
    "        is_sitting = (\n",
    "            abs(knee_y - hip_y) < 0.15 and        # Knees close to hip level\n",
    "            ankle_y > knee_y and                   # Ankles below knees\n",
    "            (left_knee_angle < 140 or              # Bent knees\n",
    "             right_knee_angle < 140)\n",
    "        )\n",
    "\n",
    "        # Enhanced squatting detection\n",
    "        is_squatting = (\n",
    "            hip_y > shoulder_y and                 # Hips below shoulders\n",
    "            knee_y > hip_y and                     # Knees below hips\n",
    "            ankle_y > knee_y and                   # Ankles below knees\n",
    "            left_knee_angle < 120 and              # Bent knees\n",
    "            right_knee_angle < 120                 # Bent knees\n",
    "        )\n",
    "\n",
    "        # Check for raised arms\n",
    "        arms_raised = self._are_arms_raised(landmarks)\n",
    "\n",
    "        # Determine posture with confidence levels\n",
    "        if is_standing:\n",
    "            return \"standing with raised arms\" if arms_raised else \"standing\"\n",
    "        elif is_squatting:\n",
    "            return \"squatting\"\n",
    "        elif is_sitting:\n",
    "            return \"sitting\"\n",
    "        else:\n",
    "            # Calculate closest match based on conditions\n",
    "            standing_score = (\n",
    "                (1 if knee_y > hip_y else 0) +\n",
    "                (1 if ankle_y > knee_y else 0) +\n",
    "                (1 if spine_alignment < 0.2 else 0) +\n",
    "                (1 if leg_alignment > 0.3 else 0) +\n",
    "                (1 if left_knee_angle > 160 else 0) +\n",
    "                (1 if right_knee_angle > 160 else 0)\n",
    "            ) / 6.0\n",
    "\n",
    "            if standing_score > 0.5:\n",
    "                return \"standing\"  # Partial standing detection\n",
    "\n",
    "            return \"undefined posture\"\n",
    "\n",
    "    def _are_arms_raised(self, landmarks):\n",
    "        \"\"\"Improved detection of raised arms\"\"\"\n",
    "        left_wrist = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "        right_wrist = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_WRIST]\n",
    "        left_shoulder = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        left_elbow = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_ELBOW]\n",
    "        right_elbow = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_ELBOW]\n",
    "\n",
    "        # Check both elbow and wrist positions relative to shoulders\n",
    "        left_arm_raised = (left_wrist.y < left_shoulder.y - 0.1 or \n",
    "                          left_elbow.y < left_shoulder.y - 0.1)\n",
    "        right_arm_raised = (right_wrist.y < right_shoulder.y - 0.1 or \n",
    "                           right_elbow.y < right_shoulder.y - 0.1)\n",
    "\n",
    "        return left_arm_raised or right_arm_raised\n",
    "\n",
    "    def _calculate_hip_knee_ratio(self, left_hip, right_hip, left_knee, right_knee):\n",
    "        \"\"\"Calculate the ratio between hip and knee positions\"\"\"\n",
    "        hip_y = (left_hip.y + right_hip.y) / 2\n",
    "        knee_y = (left_knee.y + right_knee.y) / 2\n",
    "        return abs(knee_y - hip_y)\n",
    "\n",
    "    def _calculate_angle(self, p1, p2, p3):\n",
    "        \"\"\"Calculate angle between three points\"\"\"\n",
    "        v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])\n",
    "        v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])\n",
    "\n",
    "        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n",
    "        return np.degrees(angle)\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_bounding_box(self, landmarks, model_type):\n",
    "        \"\"\"Calculate bounding box for detected pose\"\"\"\n",
    "        if model_type == 'mediapipe':\n",
    "            points = [(lm.x, lm.y) for lm in landmarks.landmark]\n",
    "        else:  # YOLO\n",
    "            points = [(x/self.current_frame_shape[1], y/self.current_frame_shape[0]) \n",
    "                     for x, y, _ in landmarks]\n",
    "\n",
    "        x_coords = [p[0] for p in points]\n",
    "        y_coords = [p[1] for p in points]\n",
    "\n",
    "        # Calculate normalized coordinates\n",
    "        x_min, x_max = min(x_coords), max(x_coords)\n",
    "        y_min, y_max = min(y_coords), max(y_coords)\n",
    "\n",
    "        # Convert to pixel coordinates\n",
    "        return {\n",
    "            'x1': int(x_min * self.current_frame_shape[1]),\n",
    "            'y1': int(y_min * self.current_frame_shape[0]),\n",
    "            'x2': int(x_max * self.current_frame_shape[1]),\n",
    "            'y2': int(y_max * self.current_frame_shape[0])\n",
    "        }\n",
    "\n",
    "    def draw_action_box(self, frame, bbox, action):\n",
    "        \"\"\"Draw bounding box and action label on frame\"\"\"\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, \n",
    "                     (bbox['x1'], bbox['y1']), \n",
    "                     (bbox['x2'], bbox['y2']), \n",
    "                     (0, 255, 0), 2)\n",
    "\n",
    "        # Draw action label\n",
    "        label_size = cv2.getTextSize(action, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n",
    "        cv2.rectangle(frame,\n",
    "                     (bbox['x1'], bbox['y1'] - label_size[1] - 10),\n",
    "                     (bbox['x1'] + label_size[0], bbox['y1']),\n",
    "                     (0, 255, 0), -1)\n",
    "        cv2.putText(frame, action,\n",
    "                   (bbox['x1'], bbox['y1'] - 5),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Enhanced main function with analytics and error handling\"\"\"\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Initialize detector\n",
    "        detector = PoseDetector()\n",
    "        logging.info(\"Pose detector initialized successfully\")\n",
    "        \n",
    "        # Initialize video capture\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            raise RuntimeError(\"Could not open webcam\")\n",
    "        \n",
    "        logging.info(\"Video capture started\")\n",
    "        \n",
    "        # Main processing loop\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                logging.warning(\"Failed to capture frame\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Process frame\n",
    "                processed_frame = detector.process_frame(frame)\n",
    "                if processed_frame is None:\n",
    "                    continue\n",
    "                \n",
    "                # Display frame\n",
    "                cv2.imshow('Pose Detection', processed_frame)\n",
    "                \n",
    "                # Handle keyboard input\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    logging.info(\"User requested exit\")\n",
    "                    break\n",
    "                elif key == ord('r'):\n",
    "                    detector.toggle_recording()\n",
    "                elif key == ord('s'):\n",
    "                    detector.save_frame()\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing frame: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fatal error: {str(e)}\")\n",
    "        \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        if cap is not None:\n",
    "            cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Generate final analytics\n",
    "        detector.analytics.generate_analytics_report()\n",
    "        logging.info(\"Analytics report generated\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34273ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c9b950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 13:34:56,386 - INFO - Pose detector initialized successfully\n",
      "2024-12-14 13:34:59,432 - INFO - Video capture started\n",
      "C:\\Users\\kitmi\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "2024-12-14 13:35:05,100 - WARNING - Low confidence detection: 0.49\n",
      "2024-12-14 13:35:05,165 - WARNING - Low confidence detection: 0.49\n",
      "2024-12-14 13:35:05,230 - WARNING - Low confidence detection: 0.48\n",
      "2024-12-14 13:35:05,285 - WARNING - Low confidence detection: 0.47\n",
      "2024-12-14 13:35:05,331 - WARNING - Low confidence detection: 0.47\n",
      "2024-12-14 13:35:05,398 - WARNING - Low confidence detection: 0.46\n",
      "2024-12-14 13:35:05,448 - WARNING - Low confidence detection: 0.46\n",
      "2024-12-14 13:35:05,529 - WARNING - Low confidence detection: 0.47\n",
      "2024-12-14 13:35:05,585 - WARNING - Low confidence detection: 0.46\n",
      "2024-12-14 13:35:05,631 - WARNING - Low confidence detection: 0.46\n",
      "2024-12-14 13:35:05,698 - WARNING - Low confidence detection: 0.46\n",
      "2024-12-14 13:35:05,748 - WARNING - Low confidence detection: 0.46\n",
      "2024-12-14 13:35:05,815 - WARNING - Low confidence detection: 0.45\n",
      "2024-12-14 13:35:05,865 - WARNING - Low confidence detection: 0.45\n",
      "2024-12-14 13:35:05,928 - WARNING - Low confidence detection: 0.45\n",
      "2024-12-14 13:35:05,986 - WARNING - Low confidence detection: 0.44\n",
      "2024-12-14 13:35:06,031 - WARNING - Low confidence detection: 0.44\n",
      "2024-12-14 13:35:06,098 - WARNING - Low confidence detection: 0.44\n",
      "2024-12-14 13:35:06,148 - WARNING - Low confidence detection: 0.44\n",
      "2024-12-14 13:35:06,211 - WARNING - Low confidence detection: 0.44\n",
      "2024-12-14 13:35:06,264 - WARNING - Low confidence detection: 0.43\n",
      "2024-12-14 13:35:06,314 - WARNING - Low confidence detection: 0.43\n",
      "2024-12-14 13:35:06,379 - WARNING - Low confidence detection: 0.43\n",
      "2024-12-14 13:35:06,431 - WARNING - Low confidence detection: 0.43\n",
      "2024-12-14 13:35:06,487 - WARNING - Low confidence detection: 0.43\n",
      "2024-12-14 13:35:06,547 - WARNING - Low confidence detection: 0.43\n",
      "2024-12-14 13:35:06,598 - WARNING - Low confidence detection: 0.43\n",
      "2024-12-14 13:35:06,647 - WARNING - Low confidence detection: 0.43\n",
      "2024-12-14 13:35:06,714 - WARNING - Low confidence detection: 0.43\n",
      "2024-12-14 13:35:06,764 - WARNING - Low confidence detection: 0.43\n",
      "2024-12-14 13:35:06,832 - WARNING - Low confidence detection: 0.43\n",
      "2024-12-14 13:35:06,881 - WARNING - Low confidence detection: 0.43\n",
      "2024-12-14 13:35:06,930 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:06,998 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:07,047 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:07,110 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:07,164 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:07,214 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:07,263 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:07,331 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:07,390 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:07,444 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:07,497 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:07,547 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:07,614 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:07,664 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:07,713 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:07,781 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:07,831 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:07,896 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:07,947 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:07,996 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,063 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,114 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,163 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,230 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,281 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,347 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,397 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,462 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,514 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,564 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,629 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,680 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,730 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,797 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,847 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,913 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:08,963 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:09,028 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:09,080 - WARNING - Low confidence detection: 0.41\n",
      "2024-12-14 13:35:09,130 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:09,196 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:09,246 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:09,314 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:09,364 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:09,428 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:09,480 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:09,530 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:09,596 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:09,646 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:09,712 - WARNING - Low confidence detection: 0.42\n",
      "2024-12-14 13:35:09,713 - INFO - User requested exit\n",
      "2024-12-14 13:35:10,444 - INFO - Analytics report generated\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import json\n",
    "import threading\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import csv\n",
    "import time\n",
    "\n",
    "class PoseAnalytics:\n",
    "    \"\"\"Analytics component for tracking and analyzing pose detection metrics\"\"\"\n",
    "    def __init__(self, save_dir='analytics'):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Initialize analytics storage\n",
    "        self.session_data = {\n",
    "            'timestamps': [],\n",
    "            'postures': [],\n",
    "            'confidence_scores': [],\n",
    "            'processing_times': [],\n",
    "            'fps_values': []\n",
    "        }\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            filename=self.save_dir / 'pose_detection.log',\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        \n",
    "        # Initialize real-time metrics\n",
    "        self.current_metrics = {\n",
    "            'session_start': datetime.now(),\n",
    "            'poses_detected': 0,\n",
    "            'average_fps': 0.0\n",
    "        }\n",
    "    \n",
    "    def update_metrics(self, posture, confidence, process_time, fps):\n",
    "        \"\"\"Update analytics with new frame data\"\"\"\n",
    "        timestamp = datetime.now()\n",
    "        \n",
    "        # Update session data\n",
    "        self.session_data['timestamps'].append(timestamp)\n",
    "        self.session_data['postures'].append(posture)\n",
    "        self.session_data['confidence_scores'].append(confidence)\n",
    "        self.session_data['processing_times'].append(process_time)\n",
    "        self.session_data['fps_values'].append(fps)\n",
    "        \n",
    "        # Update real-time metrics\n",
    "        self.current_metrics['poses_detected'] += 1\n",
    "        self.current_metrics['average_fps'] = np.mean(self.session_data['fps_values'])\n",
    "        \n",
    "        # Log significant events\n",
    "        if confidence < 0.5:\n",
    "            logging.warning(f\"Low confidence detection: {confidence:.2f}\")\n",
    "    \n",
    "    def _create_posture_distribution_plot(self, df):\n",
    "        \"\"\"Create and save posture distribution plot\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        posture_counts = df['postures'].value_counts()\n",
    "        sns.barplot(x=posture_counts.values, y=posture_counts.index)\n",
    "        plt.title('Posture Distribution')\n",
    "        plt.xlabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.save_dir / 'posture_distribution.png')\n",
    "        plt.close()\n",
    "\n",
    "    def _create_performance_plot(self, df):\n",
    "        \"\"\"Create and save performance metrics plot\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df['timestamps'], df['fps_values'], label='FPS')\n",
    "        plt.plot(df['timestamps'], df['processing_times'], label='Processing Time')\n",
    "        plt.title('Performance Metrics Over Time')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.save_dir / 'performance_metrics.png')\n",
    "        plt.close()\n",
    "\n",
    "    def _create_confidence_plot(self, df):\n",
    "        \"\"\"Create and save confidence scores plot\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df['timestamps'], df['confidence_scores'])\n",
    "        plt.title('Detection Confidence Over Time')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Confidence Score')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.save_dir / 'confidence_scores.png')\n",
    "        plt.close()\n",
    "\n",
    "    def generate_analytics_report(self):\n",
    "        \"\"\"Generate comprehensive analytics report\"\"\"\n",
    "        if not self.session_data['timestamps']:\n",
    "            logging.warning(\"No data collected for analytics report\")\n",
    "            return\n",
    "\n",
    "        # Convert data to DataFrame\n",
    "        df = pd.DataFrame(self.session_data)\n",
    "        \n",
    "        # Generate visualizations\n",
    "        self._create_posture_distribution_plot(df)\n",
    "        self._create_performance_plot(df)\n",
    "        self._create_confidence_plot(df)\n",
    "        \n",
    "        # Generate summary statistics\n",
    "        summary = self._generate_summary_stats(df)\n",
    "        \n",
    "        # Save report\n",
    "        self._save_analytics_report(df, summary)\n",
    "    \n",
    "    def _generate_summary_stats(self, df):\n",
    "        \"\"\"Generate summary statistics from the session data\"\"\"\n",
    "        summary = {\n",
    "            'total_frames': len(df),\n",
    "            'average_fps': df['fps_values'].mean(),\n",
    "            'average_confidence': df['confidence_scores'].mean(),\n",
    "            'most_common_posture': df['postures'].mode().iloc[0] if not df['postures'].empty else 'None',\n",
    "            'session_duration': str(datetime.now() - self.current_metrics['session_start'])\n",
    "        }\n",
    "        return summary\n",
    "\n",
    "    def _save_analytics_report(self, df, summary):\n",
    "        \"\"\"Save analytics report to file\"\"\"\n",
    "        report_path = self.save_dir / f'analytics_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt'\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"=== Pose Detection Analytics Report ===\\n\\n\")\n",
    "            for key, value in summary.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "class PoseDetector:\n",
    "    def __init__(self):\n",
    "        # MediaPipe initialization\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.7,\n",
    "            model_complexity=2\n",
    "        )\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_drawing_styles = mp.solutions.drawing_styles\n",
    "        \n",
    "        # Initialize analytics\n",
    "        self.analytics = PoseAnalytics()\n",
    "        \n",
    "        # Performance monitoring\n",
    "        self.fps_history = deque(maxlen=30)\n",
    "        self.detection_history = deque(maxlen=30)\n",
    "        self.joint_consistency = deque(maxlen=30)\n",
    "        \n",
    "        # Movement tracking\n",
    "        self.previous_landmarks = None\n",
    "        self.movement_history = deque(maxlen=10)\n",
    "        \n",
    "        # Confidence tracking\n",
    "        self.confidence_history = deque(maxlen=10)\n",
    "        self.low_confidence_count = 0\n",
    "        self.max_low_confidence_attempts = 3\n",
    "        \n",
    "        # Threshold values\n",
    "        self.movement_threshold = 0.02  # Normalized movement threshold\n",
    "        self.head_movement_threshold = 0.015\n",
    "        self.shoulder_movement_threshold = 0.018\n",
    "        self.squat_threshold = 0.25\n",
    "        self.standing_hip_knee_ratio = 0.9\n",
    "        self.sitting_hip_knee_ratio = 0.4\n",
    "        \n",
    "        # Confidence thresholds\n",
    "        self.low_confidence_threshold = 0.5\n",
    "        self.very_low_confidence_threshold = 0.3\n",
    "        \n",
    "        # Movement smoothing\n",
    "        self.shoulder_positions = deque(maxlen=5)\n",
    "        self.head_positions = deque(maxlen=5)\n",
    "        \n",
    "        # Posture change tracking\n",
    "        self.last_posture_change_time = 0\n",
    "        self.posture_change_interval = 0.5  # 1 seconds between posture changes\n",
    "        self.current_posture = None\n",
    "        \n",
    "        \n",
    "        # Initialize other attributes\n",
    "        self.current_frame_shape = None\n",
    "        self.recording = False\n",
    "        self.video_writer = None\n",
    "        self.output_dir = Path('output')\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process a single frame for pose detection with improved error handling and confidence tracking\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            if frame is None or frame.size == 0:\n",
    "                logging.error(\"Invalid frame received\")\n",
    "                return frame\n",
    "\n",
    "            self.current_frame_shape = frame.shape\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Process with MediaPipe\n",
    "            results = self.pose.process(rgb_frame)\n",
    "            landmarks = None\n",
    "            confidence = 0.0\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                landmarks = results.pose_landmarks\n",
    "                confidence = np.mean([lm.visibility for lm in landmarks.landmark])\n",
    "                \n",
    "                # Update confidence history\n",
    "                self.confidence_history.append(confidence)\n",
    "                \n",
    "                # Check confidence levels\n",
    "                confidence_status = self._check_confidence(confidence)\n",
    "                \n",
    "                if confidence_status == \"low\":\n",
    "                    # Add low confidence warning to frame\n",
    "                    cv2.putText(frame, \"LOW CONFIDENCE: Move back\", \n",
    "                                (10, self.current_frame_shape[0] - 50), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "                \n",
    "                if confidence_status == \"very_low\":\n",
    "                    # Add very low confidence warning to frame\n",
    "                    cv2.putText(frame, \"VERY LOW CONFIDENCE: Please reposition!\", \n",
    "                                (10, self.current_frame_shape[0] - 50), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "                    \n",
    "                    # Optional: Add blinking effect\n",
    "                    if int(time.time() * 2) % 2 == 0:\n",
    "                        frame = cv2.addWeighted(frame, 0.5, np.zeros_like(frame), 0.5, 0)\n",
    "\n",
    "                try:\n",
    "                    # Detect posture\n",
    "                    posture = self.detect_posture(landmarks)\n",
    "                    self.current_posture = posture\n",
    "                    self.current_confidence = confidence\n",
    "                    self.current_landmarks = landmarks\n",
    "\n",
    "                    # Calculate and draw bounding box\n",
    "                    bbox = self.calculate_bounding_box(landmarks, model_type=\"mediapipe\")\n",
    "                    self.draw_action_box(frame, bbox, posture)\n",
    "\n",
    "                    # Draw landmarks\n",
    "                    self.mp_drawing.draw_landmarks(\n",
    "                        frame,\n",
    "                        landmarks,\n",
    "                        self.mp_pose.POSE_CONNECTIONS,\n",
    "                        landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "                    )\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    process_time = time.time() - start_time\n",
    "                    fps = 1 / process_time if process_time > 0 else 0\n",
    "                    self.fps_history.append(fps)\n",
    "\n",
    "                    # Update analytics\n",
    "                    self.analytics.update_metrics(\n",
    "                        posture=posture,\n",
    "                        confidence=confidence,\n",
    "                        process_time=process_time,\n",
    "                        fps=fps\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing landmarks: {str(e)}\")\n",
    "\n",
    "            # Draw performance metrics\n",
    "            self._draw_performance_metrics(frame, fps if 'fps' in locals() else 0, confidence)\n",
    "\n",
    "            # Handle recording if active\n",
    "            if self.recording and self.video_writer is not None:\n",
    "                try:\n",
    "                    self.video_writer.write(frame)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error writing video frame: {str(e)}\")\n",
    "\n",
    "            return frame\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in process_frame: {str(e)}\")\n",
    "            return frame\n",
    "\n",
    "    def _draw_performance_metrics(self, frame, fps, confidence):\n",
    "        \"\"\"Draw performance metrics on frame\"\"\"\n",
    "        # Calculate average FPS\n",
    "        avg_fps = np.mean(self.fps_history) if self.fps_history else 0\n",
    "        \n",
    "        # Draw metrics\n",
    "        metrics_text = [\n",
    "            f\"FPS: {avg_fps:.1f}\",\n",
    "            f\"Confidence: {confidence:.2f}\",\n",
    "            f\"Posture: {self.current_posture}\"\n",
    "        ]\n",
    "        \n",
    "        y_position = 30\n",
    "        for text in metrics_text:\n",
    "            cv2.putText(frame, text, (10, y_position),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            y_position += 25\n",
    "\n",
    "    def toggle_recording(self):\n",
    "        \"\"\"Toggle video recording\"\"\"\n",
    "        if not self.recording:\n",
    "            # Start recording\n",
    "            filename = self.output_dir / f'pose_recording_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.avi'\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "            self.video_writer = cv2.VideoWriter(str(filename), fourcc, 20.0, \n",
    "                                              (self.current_frame_shape[1], self.current_frame_shape[0]))\n",
    "            self.recording = True\n",
    "            logging.info(\"Recording started\")\n",
    "        else:\n",
    "            # Stop recording\n",
    "            if self.video_writer is not None:\n",
    "                self.video_writer.release()\n",
    "                self.video_writer = None\n",
    "            self.recording = False\n",
    "            logging.info(\"Recording stopped\")\n",
    "\n",
    "    def save_frame(self):\n",
    "        \"\"\"Save current frame as image\"\"\"\n",
    "        if self.current_frame_shape is not None:\n",
    "            filename = self.output_dir / f'pose_frame_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.jpg'\n",
    "            cv2.imwrite(str(filename), self.current_frame)\n",
    "            logging.info(f\"Frame saved: {filename}\")\n",
    "\n",
    "    def _load_gesture_patterns(self):\n",
    "        \"\"\"Load predefined gesture patterns\"\"\"\n",
    "        return {\n",
    "            'wave': {'sequence': ['standing with raised arms', 'standing'], 'threshold': 0.8},\n",
    "            'squat': {'sequence': ['standing', 'squatting', 'standing'], 'threshold': 0.9}\n",
    "        }\n",
    "\n",
    "    def _detect_head_movement(self, landmarks):\n",
    "        \"\"\"\n",
    "        Detect head movement patterns\n",
    "        Returns: string indicating head movement type\n",
    "        \"\"\"\n",
    "        nose = landmarks.landmark[self.mp_pose.PoseLandmark.NOSE]\n",
    "        left_ear = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_EAR]\n",
    "        right_ear = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_EAR]\n",
    "        \n",
    "        # Store current head position\n",
    "        head_pos = (nose.x, nose.y)\n",
    "        self.head_positions.append(head_pos)\n",
    "        \n",
    "        if len(self.head_positions) < 2:\n",
    "            return \"\"\n",
    "            \n",
    "        # Calculate movement\n",
    "        prev_pos = self.head_positions[-2]\n",
    "        dx = head_pos[0] - prev_pos[0]\n",
    "        dy = head_pos[1] - prev_pos[1]\n",
    "        \n",
    "        # Calculate ear position relative to nose for head rotation\n",
    "        left_ear_dist = abs(left_ear.x - nose.x)\n",
    "        right_ear_dist = abs(right_ear.x - nose.x)\n",
    "        ear_ratio = left_ear_dist / right_ear_dist if right_ear_dist > 0 else 1.0\n",
    "        \n",
    "        # Detect movements\n",
    "        if abs(dx) > self.head_movement_threshold:\n",
    "            if ear_ratio > 1.2:\n",
    "                return \"turning head left\"\n",
    "            elif ear_ratio < 0.8:\n",
    "                return \"turning head right\"\n",
    "            else:\n",
    "                return \"moving head horizontally\"\n",
    "        elif abs(dy) > self.head_movement_threshold:\n",
    "            return \"nodding head\"\n",
    "            \n",
    "        return \"\"\n",
    "\n",
    "    def _detect_shoulder_movement(self, landmarks):\n",
    "        \"\"\"\n",
    "        More precise shoulder movement detection\n",
    "        Returns: string indicating shoulder movement type\n",
    "        \"\"\"\n",
    "        left_shoulder = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        left_elbow = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_ELBOW]\n",
    "        right_elbow = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_ELBOW]\n",
    "        \n",
    "        # Store current shoulder positions\n",
    "        shoulder_pos = (\n",
    "            (left_shoulder.x, left_shoulder.y),\n",
    "            (right_shoulder.x, right_shoulder.y)\n",
    "        )\n",
    "        self.shoulder_positions.append(shoulder_pos)\n",
    "        \n",
    "        if len(self.shoulder_positions) < 2:\n",
    "            return \"\"\n",
    "            \n",
    "        # Calculate movement\n",
    "        prev_pos = self.shoulder_positions[-2]\n",
    "        left_dx = shoulder_pos[0][0] - prev_pos[0][0]\n",
    "        left_dy = shoulder_pos[0][1] - prev_pos[0][1]\n",
    "        right_dx = shoulder_pos[1][0] - prev_pos[1][0]\n",
    "        right_dy = shoulder_pos[1][1] - prev_pos[1][1]\n",
    "        \n",
    "        # Calculate elbow positions for more context\n",
    "        left_elbow_pos = (left_elbow.x, left_elbow.y)\n",
    "        right_elbow_pos = (right_elbow.x, right_elbow.y)\n",
    "        \n",
    "        # Enhanced shoulder movement detection\n",
    "        # Check vertical and horizontal movements with context\n",
    "        vertical_diff_left = abs(left_dy)\n",
    "        vertical_diff_right = abs(right_dy)\n",
    "        horizontal_diff_left = abs(left_dx)\n",
    "        horizontal_diff_right = abs(right_dx)\n",
    "        \n",
    "        # Shoulder shrug detection\n",
    "        if (vertical_diff_left > self.shoulder_movement_threshold and \n",
    "            vertical_diff_right > self.shoulder_movement_threshold and \n",
    "            abs(left_dy - right_dy) < 0.02):  # Almost simultaneous\n",
    "            return \"shrugging shoulders\"\n",
    "        \n",
    "        # Asymmetric shoulder movement\n",
    "        if vertical_diff_left > self.shoulder_movement_threshold * 1.5 and \\\n",
    "           vertical_diff_right < self.shoulder_movement_threshold:\n",
    "            return \"raising left shoulder\"\n",
    "        \n",
    "        if vertical_diff_right > self.shoulder_movement_threshold * 1.5 and \\\n",
    "           vertical_diff_left < self.shoulder_movement_threshold:\n",
    "            return \"raising right shoulder\"\n",
    "        \n",
    "        # Rotational movement\n",
    "        if horizontal_diff_left > self.shoulder_movement_threshold and \\\n",
    "           horizontal_diff_right > self.shoulder_movement_threshold:\n",
    "            return \"rotating shoulders\"\n",
    "        \n",
    "        return \"\"\n",
    "\n",
    "    def detect_posture(self, landmarks):\n",
    "        \"\"\"Enhanced posture detection with improved sitting and squatting logic\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Only update posture every 5 seconds\n",
    "        if (current_time - self.last_posture_change_time < self.posture_change_interval \n",
    "            and self.current_posture is not None):\n",
    "            return self.current_posture\n",
    "\n",
    "        # Get base posture\n",
    "        base_posture = self._detect_base_posture(landmarks)\n",
    "        \n",
    "        # Detect head and shoulder movements\n",
    "        head_movement = self._detect_head_movement(landmarks)\n",
    "        shoulder_movement = self._detect_shoulder_movement(landmarks)\n",
    "        \n",
    "        # Determine sitting type if sitting\n",
    "        if 'sitting' in base_posture:\n",
    "            base_posture = f\"sitting on floor\"\n",
    "        \n",
    "        # Combine movements with base posture\n",
    "        movements = []\n",
    "        if head_movement:\n",
    "            movements.append(head_movement)\n",
    "        if shoulder_movement:\n",
    "            movements.append(shoulder_movement)\n",
    "            \n",
    "        if movements:\n",
    "            final_posture = f\"{base_posture} while {' and '.join(movements)}\"\n",
    "        else:\n",
    "            final_posture = base_posture\n",
    "        \n",
    "        # Update tracking\n",
    "        self.current_posture = final_posture\n",
    "        self.last_posture_change_time = current_time\n",
    "        \n",
    "        return final_posture\n",
    "\n",
    "    def _detect_base_posture(self, landmarks):\n",
    "        \"\"\"Detect base posture (standing, sitting, squatting)\"\"\"\n",
    "        # Get relevant landmark points\n",
    "        left_shoulder = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        left_hip = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_HIP]\n",
    "        right_hip = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_HIP]\n",
    "        left_knee = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_KNEE]\n",
    "        right_knee = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_KNEE]\n",
    "        left_ankle = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_ANKLE]\n",
    "        right_ankle = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_ANKLE]\n",
    "        \n",
    "        # Calculate vertical positions and angles\n",
    "        hip_y = (left_hip.y + right_hip.y) / 2\n",
    "        knee_y = (left_knee.y + right_knee.y) / 2\n",
    "        ankle_y = (left_ankle.y + right_ankle.y) / 2\n",
    "        shoulder_y = (left_shoulder.y + right_shoulder.y) / 2\n",
    "        \n",
    "        # Calculate joint angles\n",
    "        left_knee_angle = self._calculate_angle(\n",
    "            (left_hip.x, left_hip.y),\n",
    "            (left_knee.x, left_knee.y),\n",
    "            (left_ankle.x, left_ankle.y)\n",
    "        )\n",
    "        right_knee_angle = self._calculate_angle(\n",
    "            (right_hip.x, right_hip.y),\n",
    "            (right_knee.x, right_knee.y),\n",
    "            (right_ankle.x, right_ankle.y)\n",
    "        )\n",
    "        \n",
    "        # Calculate hip angle\n",
    "        hip_angle = self._calculate_angle(\n",
    "            (shoulder_y, 0),\n",
    "            (hip_y, 0),\n",
    "            (knee_y, 0)\n",
    "        )\n",
    "        \n",
    "        # Enhanced posture detection with confidence scores\n",
    "        standing_score = self._calculate_standing_score(\n",
    "            hip_y, knee_y, ankle_y, shoulder_y,\n",
    "            left_knee_angle, right_knee_angle\n",
    "        )\n",
    "        sitting_score = self._calculate_sitting_score(\n",
    "            hip_y, knee_y, ankle_y,\n",
    "            left_knee_angle, right_knee_angle\n",
    "        )\n",
    "        squatting_score = self.calculate_squat_score(\n",
    "            hip_y, knee_y, ankle_y, shoulder_y,\n",
    "            left_knee_angle, right_knee_angle\n",
    "        )\n",
    "\n",
    "        # Adjust threshold for posture determination\n",
    "        scores = {\n",
    "            \"standing\": standing_score,\n",
    "            \"sitting\": sitting_score,\n",
    "            \"squatting\": squatting_score\n",
    "        }\n",
    "\n",
    "        # More nuanced posture selection\n",
    "        base_posture = max(scores.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "\n",
    "        # Additional checks to prevent false positives\n",
    "        if base_posture == \"standing\":\n",
    "            # Check if it might actually be a squat\n",
    "            if squatting_score > 0.00000001 or scores[\"squatting\"] > scores[\"standing\"]:\n",
    "                base_posture = \"squatting\"\n",
    "        \n",
    "        if base_posture == \"sitting\":\n",
    "            # Check if it might actually be a squat\n",
    "            if squatting_score > 0.00000001 or scores[\"squatting\"] > scores[\"sitting\"]:\n",
    "                base_posture = \"squatting\"\n",
    "        \n",
    "            # Check for raised arms\n",
    "        if self._are_arms_raised(landmarks):\n",
    "            base_posture += \" with raised arms\"\n",
    "\n",
    "        return base_posture\n",
    "\n",
    "    def _calculate_standing_score(self, hip_y, knee_y, ankle_y, shoulder_y,\n",
    "                                 left_knee_angle, right_knee_angle):\n",
    "\n",
    "        score = 0.0\n",
    "\n",
    "        # 1. Vertical Alignment Check (40% of score)\n",
    "        if shoulder_y > hip_y > knee_y > ankle_y:\n",
    "            score += 0.4\n",
    "\n",
    "        # 2. Knee Angle Check (30% of score)\n",
    "        knee_angle_score = 0.0\n",
    "        if 160 <= left_knee_angle <= 180 and 160 <= right_knee_angle <= 180:\n",
    "            # Both knees in fully extended position\n",
    "            knee_angle_score = 0.3\n",
    "        elif 160 <= left_knee_angle <= 180 or 160 <= right_knee_angle <= 180:\n",
    "            # At least one knee in fully extended position\n",
    "            knee_angle_score = 0.15\n",
    "        score += knee_angle_score\n",
    "\n",
    "        # 3. Spine Alignment Check (30% of score)\n",
    "        spine_alignment_score = 0.0\n",
    "        if abs(shoulder_y - hip_y) < 0.1:\n",
    "            # Shoulder and hip vertically aligned\n",
    "            spine_alignment_score = 0.3\n",
    "        elif abs(shoulder_y - hip_y) < 0.2:\n",
    "            # Moderate spine alignment\n",
    "            spine_alignment_score = 0.15\n",
    "        score += spine_alignment_score\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _calculate_sitting_score(self, hip_y, knee_y, ankle_y,\n",
    "                               left_knee_angle, right_knee_angle):\n",
    "        \"\"\"Calculate confidence score for sitting posture\"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # 1. Hip-Knee Alignment Check (40% of score)\n",
    "        alignment_score = 0.0\n",
    "\n",
    "        # Instead of comparing to floor height, check relative positioning\n",
    "        if abs(knee_y - hip_y) < 0.1:  # Knees close to hip level\n",
    "            alignment_score = 0.4\n",
    "        score += alignment_score\n",
    "\n",
    "        # 2. Knee Angle Check (30% of score)\n",
    "        knee_angle_score = 0.0\n",
    "        if 90 <= left_knee_angle <= 120 and 90 <= right_knee_angle <= 120:\n",
    "            # Both knees in proper flexion range for floor sitting\n",
    "            knee_angle_score = 0.3\n",
    "        elif 90 <= left_knee_angle <= 120 or 90 <= right_knee_angle <= 120:\n",
    "            # At least one knee in proper flexion range\n",
    "            knee_angle_score = 0.15\n",
    "        score += knee_angle_score\n",
    "\n",
    "        # 3. Ankle Position Check (30% of score)\n",
    "        ankle_score = 0.0\n",
    "        if abs(ankle_y - knee_y) < 0.1:  # Ankles close to knee level\n",
    "            ankle_score = 0.3\n",
    "        score += ankle_score\n",
    "\n",
    "        return min(score, 1.0)  # Ensure score doesn't exceed 1.0\n",
    "\n",
    "    def calculate_squat_score(self, hip_y, knee_y, ankle_y, shoulder_y,\n",
    "                               left_knee_angle, right_knee_angle):\n",
    "        # Basic score calculation focusing on key body alignments\n",
    "        score = 0.0\n",
    "\n",
    "        # 1. Vertical Alignment Check (40% of score)\n",
    "        if shoulder_y > hip_y > knee_y > ankle_y:\n",
    "            score += 0.4\n",
    "\n",
    "        # 2. Squat Depth Check (30% of score)\n",
    "        depth = hip_y - ankle_y\n",
    "        if depth > 0.5:  # Deep squat\n",
    "            score += 0.3\n",
    "        elif depth > 0.3:  # Moderate squat\n",
    "            score += 0.2\n",
    "        elif depth > 0.2:  # Partial squat\n",
    "            score += 0.1\n",
    "\n",
    "        # 3. Knee Angle Check (30% of score)\n",
    "        knee_angle_score = 0.0\n",
    "        if 50 < left_knee_angle < 110 and 50 < right_knee_angle < 110:\n",
    "            knee_angle_score = 0.3\n",
    "        elif 50 < left_knee_angle < 110 or 50 < right_knee_angle < 110:\n",
    "            knee_angle_score = 0.15\n",
    "        score += knee_angle_score\n",
    "\n",
    "        return score\n",
    "    \n",
    "    def _are_arms_raised(self, landmarks):\n",
    "        \n",
    "        left_wrist = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "        right_wrist = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_WRIST]\n",
    "        left_shoulder = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        right_shoulder = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        left_elbow = landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_ELBOW]\n",
    "        right_elbow = landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_ELBOW]\n",
    "\n",
    "        # Check both elbow and wrist positions relative to shoulders\n",
    "        left_arm_raised = (left_wrist.y < left_shoulder.y - 0.1 or \n",
    "                          left_elbow.y < left_shoulder.y - 0.1)\n",
    "        right_arm_raised = (right_wrist.y < right_shoulder.y - 0.1 or \n",
    "                           right_elbow.y < right_shoulder.y - 0.1)\n",
    "\n",
    "        return left_arm_raised or right_arm_raised\n",
    "\n",
    "    def _calculate_hip_knee_ratio(self, left_hip, right_hip, left_knee, right_knee):\n",
    "        \"\"\"Calculate the ratio between hip and knee positions\"\"\"\n",
    "        hip_y = (left_hip.y + right_hip.y) / 2\n",
    "        knee_y = (left_knee.y + right_knee.y) / 2\n",
    "        return abs(knee_y - hip_y)\n",
    "\n",
    "    def _calculate_angle(self, p1, p2, p3):\n",
    "        \"\"\"Calculate angle between three points\"\"\"\n",
    "        v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])\n",
    "        v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])\n",
    "\n",
    "        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n",
    "        return np.degrees(angle)\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_bounding_box(self, landmarks, model_type):\n",
    "\n",
    "        if model_type == 'mediapipe':\n",
    "            points = [(lm.x, lm.y) for lm in landmarks.landmark]\n",
    "        else:  # YOLO\n",
    "            points = [(x/self.current_frame_shape[1], y/self.current_frame_shape[0]) \n",
    "                     for x, y, _ in landmarks]\n",
    "\n",
    "        x_coords = [p[0] for p in points]\n",
    "        y_coords = [p[1] for p in points]\n",
    "\n",
    "        # Calculate normalized coordinates\n",
    "        x_min, x_max = min(x_coords), max(x_coords)\n",
    "        y_min, y_max = min(y_coords), max(y_coords)\n",
    "\n",
    "        # Convert to pixel coordinates\n",
    "        return {\n",
    "            'x1': int(x_min * self.current_frame_shape[1]),\n",
    "            'y1': int(y_min * self.current_frame_shape[0]),\n",
    "            'x2': int(x_max * self.current_frame_shape[1]),\n",
    "            'y2': int(y_max * self.current_frame_shape[0])\n",
    "        }\n",
    "\n",
    "    def draw_action_box(self, frame, bbox, action):\n",
    "        \"\"\"Draw bounding box and action label on frame\"\"\"\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, \n",
    "                     (bbox['x1'], bbox['y1']), \n",
    "                     (bbox['x2'], bbox['y2']), \n",
    "                     (0, 255, 0), 2)\n",
    "\n",
    "        # Draw action label\n",
    "        label_size = cv2.getTextSize(action, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n",
    "        cv2.rectangle(frame,\n",
    "                     (bbox['x1'], bbox['y1'] - label_size[1] - 10),\n",
    "                     (bbox['x1'] + label_size[0], bbox['y1']),\n",
    "                     (0, 255, 0), -1)\n",
    "        cv2.putText(frame, action,\n",
    "                   (bbox['x1'], bbox['y1'] - 5),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
    "    \n",
    "    def _check_confidence(self, confidence):\n",
    "\n",
    "        if confidence > self.low_confidence_threshold:\n",
    "            # Reset low confidence counter\n",
    "            self.low_confidence_count = 0\n",
    "            return \"normal\"\n",
    "        \n",
    "        # Increment low confidence counter\n",
    "        self.low_confidence_count += 1\n",
    "        \n",
    "        if confidence <= self.very_low_confidence_threshold:\n",
    "            # Critical low confidence\n",
    "            return \"very_low\"\n",
    "        \n",
    "        # Standard low confidence\n",
    "        return \"low\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Enhanced main function with analytics and error handling\"\"\"\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Initialize detector\n",
    "        detector = PoseDetector()\n",
    "        logging.info(\"Pose detector initialized successfully\")\n",
    "        \n",
    "        # Initialize video capture\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            raise RuntimeError(\"Could not open webcam\")\n",
    "        \n",
    "        logging.info(\"Video capture started\")\n",
    "        \n",
    "        # Main processing loop\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                logging.warning(\"Failed to capture frame\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Process frame\n",
    "                processed_frame = detector.process_frame(frame)\n",
    "                if processed_frame is None:\n",
    "                    continue\n",
    "                \n",
    "                # Display frame\n",
    "                cv2.namedWindow(\"Pose Detection\", cv2.WINDOW_NORMAL)\n",
    "                cv2.setWindowProperty(\"Pose Detection\", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "                cv2.imshow('Pose Detection', processed_frame)\n",
    "                \n",
    "                # Handle keyboard input\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    logging.info(\"User requested exit\")\n",
    "                    break\n",
    "                elif key == ord('r'):\n",
    "                    detector.toggle_recording()\n",
    "                elif key == ord('s'):\n",
    "                    detector.save_frame()\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing frame: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fatal error: {str(e)}\")\n",
    "        \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        if cap is not None:\n",
    "            cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Generate final analytics\n",
    "        detector.analytics.generate_analytics_report()\n",
    "        logging.info(\"Analytics report generated\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6896ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PILImage\n",
    "from IPython.display import clear_output, display, Image\n",
    "import io\n",
    "import socket\n",
    "import numpy as np\n",
    "import time\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from PIL import ImageTk\n",
    "import threading\n",
    "import sys\n",
    "\n",
    "SERVER_IP = \"127.0.0.1\"\n",
    "SERVER_PORT = 9999\n",
    "\n",
    "class NAOCameraApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"NAO Robot Camera Feed\")\n",
    "        \n",
    "        # Make it fullscreen\n",
    "        self.root.attributes('-fullscreen', True)\n",
    "        \n",
    "        # Configure root to expand\n",
    "        self.root.grid_rowconfigure(0, weight=1)\n",
    "        self.root.grid_columnconfigure(0, weight=1)\n",
    "        \n",
    "        # Create main frame\n",
    "        self.main_frame = ttk.Frame(root)\n",
    "        self.main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))\n",
    "        \n",
    "        # Configure main_frame to expand\n",
    "        self.main_frame.grid_rowconfigure(0, weight=1)\n",
    "        self.main_frame.grid_columnconfigure(0, weight=1)\n",
    "        \n",
    "        # Create label for image display\n",
    "        self.image_label = ttk.Label(self.main_frame)\n",
    "        self.image_label.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))\n",
    "        \n",
    "        # Create status label\n",
    "        self.status_label = ttk.Label(self.main_frame, text=\"Disconnected\")\n",
    "        self.status_label.grid(row=1, column=0, pady=5)\n",
    "        \n",
    "        # Connection status\n",
    "        self.running = False\n",
    "        self.client_socket = None\n",
    "        self._quit_event = threading.Event()\n",
    "        \n",
    "        # Bind quit event to both 'q' and 'Q'\n",
    "        self.root.bind('q', self.quit_application)\n",
    "        self.root.bind('Q', self.quit_application)\n",
    "        self.root.bind('<Escape>', self.quit_application)\n",
    "        \n",
    "        # Set socket timeout to make quit more responsive\n",
    "        self.socket_timeout = 0.1\n",
    "        \n",
    "        # Start connection thread\n",
    "        self.connection_thread = threading.Thread(target=self.display_live_feed)\n",
    "        self.connection_thread.daemon = True\n",
    "        self.connection_thread.start()\n",
    "\n",
    "    def quit_application(self, event=None):\n",
    "        if self._quit_event.is_set():  # Prevent multiple quit attempts\n",
    "            return\n",
    "        self._quit_event.set()\n",
    "        self.running = False\n",
    "        \n",
    "        def force_quit():\n",
    "            if self.client_socket:\n",
    "                try:\n",
    "                    self.client_socket.shutdown(socket.SHUT_RDWR)\n",
    "                    self.client_socket.close()\n",
    "                except:\n",
    "                    pass\n",
    "            self.root.quit()\n",
    "            self.root.destroy()\n",
    "        \n",
    "        # Schedule force quit after a short delay\n",
    "        self.root.after(100, force_quit)\n",
    "\n",
    "    def update_image(self, frame):\n",
    "        if self._quit_event.is_set():\n",
    "            return\n",
    "        try:\n",
    "            pil_image = PILImage.fromarray(frame)\n",
    "            \n",
    "            screen_width = self.root.winfo_width()\n",
    "            screen_height = self.root.winfo_height()\n",
    "            \n",
    "            img_width, img_height = pil_image.size\n",
    "            aspect_ratio = img_width / img_height\n",
    "            \n",
    "            if screen_width / screen_height > aspect_ratio:\n",
    "                new_height = screen_height\n",
    "                new_width = int(screen_height * aspect_ratio)\n",
    "            else:\n",
    "                new_width = screen_width\n",
    "                new_height = int(screen_width / aspect_ratio)\n",
    "            \n",
    "            pil_image = pil_image.resize((new_width, new_height), PILImage.Resampling.LANCZOS)\n",
    "            photo = ImageTk.PhotoImage(image=pil_image)\n",
    "            \n",
    "            self.image_label.configure(image=photo)\n",
    "            self.image_label.image = photo\n",
    "        except Exception as e:\n",
    "            if not self._quit_event.is_set():\n",
    "                print(f\"Error updating image: {e}\")\n",
    "\n",
    "    def display_live_feed(self):\n",
    "        self.client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.client_socket.settimeout(self.socket_timeout)  # Set timeout for socket operations\n",
    "        self.running = True\n",
    "        \n",
    "        try:\n",
    "            self.client_socket.connect((SERVER_IP, SERVER_PORT))\n",
    "            self.status_label.configure(text=\"Connected to NAO robot stream server\")\n",
    "            \n",
    "            while self.running and not self._quit_event.is_set():\n",
    "                try:\n",
    "                    # Receive width and height\n",
    "                    width_data = self.client_socket.recv(4)\n",
    "                    if not width_data:\n",
    "                        break\n",
    "                        \n",
    "                    width = int.from_bytes(width_data, byteorder=\"big\")\n",
    "                    height = int.from_bytes(self.client_socket.recv(4), byteorder=\"big\")\n",
    "                    \n",
    "                    frame_size = width * height * 3\n",
    "                    raw_data = bytearray()\n",
    "                    remaining = frame_size\n",
    "                    \n",
    "                    while remaining > 0 and not self._quit_event.is_set():\n",
    "                        try:\n",
    "                            chunk = self.client_socket.recv(min(remaining, 4096))\n",
    "                            if not chunk:\n",
    "                                raise ConnectionError(\"Connection lost while receiving frame data\")\n",
    "                            raw_data.extend(chunk)\n",
    "                            remaining -= len(chunk)\n",
    "                        except socket.timeout:\n",
    "                            if self._quit_event.is_set():\n",
    "                                return\n",
    "                            continue\n",
    "                    \n",
    "                    if self._quit_event.is_set():\n",
    "                        return\n",
    "                        \n",
    "                    frame = np.frombuffer(raw_data, dtype=np.uint8).reshape((height, width, 3))\n",
    "                    self.root.after(0, self.update_image, frame)\n",
    "                    \n",
    "                except socket.timeout:\n",
    "                    continue\n",
    "                except ConnectionError as e:\n",
    "                    if not self._quit_event.is_set():\n",
    "                        self.status_label.configure(text=f\"Connection error: {e}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if not self._quit_event.is_set():\n",
    "                        self.status_label.configure(text=f\"Error processing frame: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "        except ConnectionRefusedError:\n",
    "            if not self._quit_event.is_set():\n",
    "                self.status_label.configure(text=\"Could not connect to server. Make sure the server is running.\")\n",
    "        except Exception as e:\n",
    "            if not self._quit_event.is_set():\n",
    "                self.status_label.configure(text=f\"Unexpected error: {e}\")\n",
    "        finally:\n",
    "            if self.client_socket:\n",
    "                try:\n",
    "                    self.client_socket.close()\n",
    "                except:\n",
    "                    pass\n",
    "            if not self._quit_event.is_set():\n",
    "                self.status_label.configure(text=\"Connection closed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = NAOCameraApp(root)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PILImage\n",
    "from IPython.display import clear_output, display, Image\n",
    "import io\n",
    "import socket\n",
    "import numpy as np\n",
    "import time\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import seaborn as sns\n",
    "from PIL import ImageTk\n",
    "import threading\n",
    "import sys\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import queue\n",
    "import pandas as pd\n",
    "\n",
    "class PoseAnalyticsWindow:\n",
    "    def __init__(self, analytics):\n",
    "        self.window = tk.Toplevel()\n",
    "        self.window.title(\"Pose Analytics\")\n",
    "        self.analytics = analytics\n",
    "        \n",
    "        # Make the analytics window smaller than main window\n",
    "        self.window.geometry(\"1000x800\")\n",
    "        \n",
    "        # Configure window to expand\n",
    "        self.window.grid_rowconfigure(0, weight=1)\n",
    "        self.window.grid_columnconfigure(0, weight=1)\n",
    "        \n",
    "        # Create notebook for different analytics tabs\n",
    "        self.notebook = ttk.Notebook(self.window)\n",
    "        self.notebook.grid(row=0, column=0, sticky='nsew', padx=5, pady=5)\n",
    "        \n",
    "        # Create tabs\n",
    "        self.realtime_tab = ttk.Frame(self.notebook)\n",
    "        self.history_tab = ttk.Frame(self.notebook)\n",
    "        self.stats_tab = ttk.Frame(self.notebook)\n",
    "        \n",
    "        self.notebook.add(self.realtime_tab, text='Real-time Analysis')\n",
    "        self.notebook.add(self.history_tab, text='History')\n",
    "        self.notebook.add(self.stats_tab, text='Statistics')\n",
    "        \n",
    "        # Initialize components\n",
    "        self.setup_realtime_tab()\n",
    "        self.setup_history_tab()\n",
    "        self.setup_stats_tab()\n",
    "        \n",
    "        # Update interval\n",
    "        self.update_interval = 100  # milliseconds\n",
    "        self.window.after(self.update_interval, self.update_displays)\n",
    "\n",
    "    def setup_realtime_tab(self):\n",
    "        # Real-time metrics frame\n",
    "        self.metrics_frame = ttk.LabelFrame(self.realtime_tab, text=\"Current Metrics\")\n",
    "        self.metrics_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # FPS and confidence display\n",
    "        self.fps_label = ttk.Label(self.metrics_frame, text=\"FPS: 0\")\n",
    "        self.fps_label.pack(side=tk.LEFT, padx=10)\n",
    "        \n",
    "        self.confidence_label = ttk.Label(self.metrics_frame, text=\"Confidence: 0%\")\n",
    "        self.confidence_label.pack(side=tk.LEFT, padx=10)\n",
    "        \n",
    "        self.posture_label = ttk.Label(self.metrics_frame, text=\"Current Posture: None\")\n",
    "        self.posture_label.pack(side=tk.LEFT, padx=10)\n",
    "        \n",
    "        # Create matplotlib figure for real-time confidence plot\n",
    "        self.rt_figure, (self.conf_ax, self.fps_ax) = plt.subplots(2, 1, figsize=(10, 6))\n",
    "        self.rt_canvas = FigureCanvasTkAgg(self.rt_figure, master=self.realtime_tab)\n",
    "        self.rt_canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "\n",
    "    def setup_history_tab(self):\n",
    "        # Controls frame\n",
    "        self.history_controls = ttk.Frame(self.history_tab)\n",
    "        self.history_controls.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        ttk.Label(self.history_controls, text=\"Time Range:\").pack(side=tk.LEFT, padx=5)\n",
    "        self.time_range = ttk.Combobox(self.history_controls, \n",
    "                                     values=['Last 1 minute', 'Last 5 minutes', 'Last 15 minutes'])\n",
    "        self.time_range.pack(side=tk.LEFT, padx=5)\n",
    "        self.time_range.set('Last 5 minutes')\n",
    "        self.time_range.bind('<<ComboboxSelected>>', self.update_history_plots)\n",
    "        \n",
    "        ttk.Button(self.history_controls, text=\"Generate Report\", \n",
    "                  command=self.analytics.generate_analytics_report).pack(side=tk.RIGHT, padx=5)\n",
    "        \n",
    "        # Create matplotlib figure for historical plots\n",
    "        self.hist_figure, (self.posture_ax, self.perf_ax) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "        self.hist_canvas = FigureCanvasTkAgg(self.hist_figure, master=self.history_tab)\n",
    "        self.hist_canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "\n",
    "    def setup_stats_tab(self):\n",
    "        self.stats_frame = ttk.Frame(self.stats_tab)\n",
    "        self.stats_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Session statistics\n",
    "        self.session_frame = ttk.LabelFrame(self.stats_frame, text=\"Session Statistics\")\n",
    "        self.session_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        self.stats_labels = {\n",
    "            'session_duration': ttk.Label(self.session_frame, text=\"Session Duration: 0:00\"),\n",
    "            'total_frames': ttk.Label(self.session_frame, text=\"Total Frames: 0\"),\n",
    "            'avg_fps': ttk.Label(self.session_frame, text=\"Average FPS: 0.0\"),\n",
    "            'avg_confidence': ttk.Label(self.session_frame, text=\"Average Confidence: 0%\"),\n",
    "        }\n",
    "        \n",
    "        for label in self.stats_labels.values():\n",
    "            label.pack(anchor='w', padx=5, pady=2)\n",
    "\n",
    "    def update_displays(self):\n",
    "        \"\"\"Update all displays with current analytics data\"\"\"\n",
    "        if not self.analytics.session_data['timestamps']:\n",
    "            self.window.after(self.update_interval, self.update_displays)\n",
    "            return\n",
    "            \n",
    "        # Update real-time metrics\n",
    "        self.update_realtime_metrics()\n",
    "        \n",
    "        # Update plots if current tab is visible\n",
    "        current_tab = self.notebook.select()\n",
    "        if current_tab == str(self.realtime_tab):\n",
    "            self.update_realtime_plots()\n",
    "        elif current_tab == str(self.history_tab):\n",
    "            self.update_history_plots()\n",
    "        elif current_tab == str(self.stats_tab):\n",
    "            self.update_statistics()\n",
    "        \n",
    "        self.window.after(self.update_interval, self.update_displays)\n",
    "\n",
    "    def update_realtime_metrics(self):\n",
    "        \"\"\"Update real-time metric labels\"\"\"\n",
    "        if self.analytics.session_data['fps_values']:\n",
    "            current_fps = self.analytics.session_data['fps_values'][-1]\n",
    "            self.fps_label.config(text=f\"FPS: {current_fps:.1f}\")\n",
    "        \n",
    "        if self.analytics.session_data['confidence_scores']:\n",
    "            current_confidence = self.analytics.session_data['confidence_scores'][-1]\n",
    "            self.confidence_label.config(text=f\"Confidence: {current_confidence:.1%}\")\n",
    "        \n",
    "        if self.analytics.session_data['postures']:\n",
    "            current_posture = self.analytics.session_data['postures'][-1]\n",
    "            self.posture_label.config(text=f\"Current Posture: {current_posture}\")\n",
    "\n",
    "    def update_realtime_plots(self):\n",
    "        \"\"\"Update real-time plots\"\"\"\n",
    "        # Clear previous plots\n",
    "        self.conf_ax.clear()\n",
    "        self.fps_ax.clear()\n",
    "        \n",
    "        # Get last 100 data points\n",
    "        timestamps = self.analytics.session_data['timestamps'][-100:]\n",
    "        confidence_scores = self.analytics.session_data['confidence_scores'][-100:]\n",
    "        fps_values = self.analytics.session_data['fps_values'][-100:]\n",
    "        \n",
    "        # Plot confidence scores\n",
    "        self.conf_ax.plot(timestamps, confidence_scores)\n",
    "        self.conf_ax.set_title('Real-time Confidence Scores')\n",
    "        self.conf_ax.set_ylim(0, 1)\n",
    "        \n",
    "        # Plot FPS\n",
    "        self.fps_ax.plot(timestamps, fps_values)\n",
    "        self.fps_ax.set_title('Real-time FPS')\n",
    "        \n",
    "        # Adjust layout and redraw\n",
    "        self.rt_figure.tight_layout()\n",
    "        self.rt_canvas.draw()\n",
    "\n",
    "    def update_history_plots(self, event=None):\n",
    "        \"\"\"Update historical plots\"\"\"\n",
    "        # Clear previous plots\n",
    "        self.posture_ax.clear()\n",
    "        self.perf_ax.clear()\n",
    "        \n",
    "        # Convert time range to minutes\n",
    "        time_range_str = self.time_range.get()\n",
    "        minutes = int(time_range_str.split()[1])\n",
    "        \n",
    "        # Create DataFrame from session data\n",
    "        df = pd.DataFrame(self.analytics.session_data)\n",
    "        \n",
    "        # Filter data by time range\n",
    "        cutoff_time = datetime.now() - pd.Timedelta(minutes=minutes)\n",
    "        df = df[df['timestamps'] > cutoff_time]\n",
    "        \n",
    "        # Plot posture distribution\n",
    "        posture_counts = df['postures'].value_counts()\n",
    "        sns.barplot(x=posture_counts.values, y=posture_counts.index, ax=self.posture_ax)\n",
    "        self.posture_ax.set_title('Posture Distribution')\n",
    "        \n",
    "        # Plot performance metrics\n",
    "        self.perf_ax.plot(df['timestamps'], df['fps_values'], label='FPS')\n",
    "        self.perf_ax.plot(df['timestamps'], df['processing_times'], label='Processing Time')\n",
    "        self.perf_ax.set_title('Performance Metrics')\n",
    "        self.perf_ax.legend()\n",
    "        \n",
    "        # Adjust layout and redraw\n",
    "        self.hist_figure.tight_layout()\n",
    "        self.hist_canvas.draw()\n",
    "\n",
    "    def update_statistics(self):\n",
    "        \"\"\"Update statistics display\"\"\"\n",
    "        # Calculate session duration\n",
    "        duration = datetime.now() - self.analytics.current_metrics['session_start']\n",
    "        \n",
    "        # Update statistics labels\n",
    "        self.stats_labels['session_duration'].config(\n",
    "            text=f\"Session Duration: {str(duration).split('.')[0]}\")\n",
    "        \n",
    "        self.stats_labels['total_frames'].config(\n",
    "            text=f\"Total Frames: {self.analytics.current_metrics['poses_detected']}\")\n",
    "        \n",
    "        self.stats_labels['avg_fps'].config(\n",
    "            text=f\"Average FPS: {self.analytics.current_metrics['average_fps']:.1f}\")\n",
    "        \n",
    "        if self.analytics.session_data['confidence_scores']:\n",
    "            avg_confidence = np.mean(self.analytics.session_data['confidence_scores'])\n",
    "            self.stats_labels['avg_confidence'].config(\n",
    "                text=f\"Average Confidence: {avg_confidence:.1%}\")\n",
    "\n",
    "class NAOPoseDetectionApp:\n",
    "    def __init__(self, root):\n",
    "        # ... (previous initialization code remains the same) ...\n",
    "        \n",
    "        # Initialize analytics\n",
    "        self.analytics = PoseAnalytics(save_dir='analytics')\n",
    "        \n",
    "        # Create analytics window\n",
    "        self.analytics_window = PoseAnalyticsWindow(self.analytics)\n",
    "        \n",
    "        # Add button to show/hide analytics window\n",
    "        self.analytics_button = ttk.Button(self.status_frame, \n",
    "                                         text=\"Toggle Analytics\", \n",
    "                                         command=self.toggle_analytics)\n",
    "        self.analytics_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "    def process_frames(self):\n",
    "        while not self._quit_event.is_set():\n",
    "            try:\n",
    "                frame = self.frame_queue.get(timeout=0.1)\n",
    "                start_time = time.time()\n",
    "                self.last_frames_time.append(start_time)\n",
    "                \n",
    "                # Process frame with pose detection\n",
    "                processed_frame, pose_data = self.pose_detector.process_frame(frame)\n",
    "                \n",
    "                # Calculate processing time and FPS\n",
    "                process_time = time.time() - start_time\n",
    "                fps = len(self.last_frames_time) / (self.last_frames_time[-1] - self.last_frames_time[0])\n",
    "                \n",
    "                # Update analytics\n",
    "                self.analytics.update_metrics(\n",
    "                    posture=pose_data.get('posture', 'Unknown'),\n",
    "                    confidence=pose_data.get('confidence', 0.0),\n",
    "                    process_time=process_time,\n",
    "                    fps=fps\n",
    "                )\n",
    "                \n",
    "                # ... (rest of the processing code remains the same) ...\n",
    "                \n",
    "            except queue.Empty:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                if not self._quit_event.is_set():\n",
    "                    logging.error(f\"Error processing frame: {e}\")\n",
    "\n",
    "    def toggle_analytics(self):\n",
    "        \"\"\"Show or hide the analytics window\"\"\"\n",
    "        if self.analytics_window.window.winfo_viewable():\n",
    "            self.analytics_window.window.withdraw()\n",
    "        else:\n",
    "            self.analytics_window.window.deiconify()\n",
    "\n",
    "    def cleanup(self):\n",
    "        if self.client_socket:\n",
    "            try:\n",
    "                self.client_socket.close()\n",
    "            except:\n",
    "                pass\n",
    "        if not self._quit_event.is_set():\n",
    "            self.status_label.configure(text=\"Connection closed\")\n",
    "            # Generate final analytics report\n",
    "            self.analytics.generate_analytics_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
